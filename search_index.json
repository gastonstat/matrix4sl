[
["index.html", "A Matrix Algebra Companion for Statistical Learning (matrix4sl) Welcome", " A Matrix Algebra Companion for Statistical Learning (matrix4sl) Gaston Sanchez (work in progress) Welcome The purpose of this book is to help you understand how statistical notions are connected to matrix algebra concepts that constantly appear around Statistical Learning methods. This book will teach you how to make the transition from a data table to a data matrix, how to think of a data matrix from a geometric perspective, and how to express—and interpret—statistical operations with vector-matrix notation. More specifically, you will learn how to express common statistical summaries (e.g. mean, variance, covariance, correlation) with vector-matrix notation. You will also learn how to see data—geometrically speaking—and how to provide a geometric interpretation to statistical summaries and related measures. How to cite this book: Sanchez, G. (2018) A Matrix Algebra Companion for Statistical Learning https://www.gastonsanchez.com/matrix4sl Give Now If you find this resource useful (which I’m pretty sure you will) please consider making a one-time donation in any amount. Your support really matters. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["preface.html", "Preface", " Preface You’ve typically heard something like this before: “in order to have a solid understanding of statistical learning methods, you need a good knowledge of matrix algebra.” Which I agree with. Matrix algebra is fundamental for a good understanding of Statistical Learning methods, Machine Learning methods, Data Mining techniques, as well as Multivariate Data Analysis methods (keep in mind that there is a considerable amount of overlap in all these fields). What you probably haven’t heard is that “knowing matrix algebra is not (really) enough.” You also need to learn how various matrix algebra concepts are connected with the ideas and notions behind statistical learning methods. In my opinion, the three main reasons for why you should bother learning about matrix algebra are: Multivariate data is commonly represented in tabular format (rows and columns). Mathematically, a data table can be treated as a matrix. Matrix algebra provides the analytical machinery and tools to manipulate and exploit values, information, and patterns of variability in data. How this book is organized? By and large, I focus on concepts like mean, variance, covariance, notions of spread, distances, and their relationships with linear combinations, projections—that sort of thing. You will learn how to express statistical concepts with vector and matrix notation. The underlying goal is to show you how to get a geometrical interpretation of common statistical operations. Data tables Data matrix Variables Transformations Duality of the data matrix Statistical Summaries Spreads and Distances Prerequisites This is NOT a book on matrix algebra, and it is also NOT a book on Statistical Learning. Instead, this book discusses general material from linear algebra that has to do with statistics. The level of exposure of the contents in this text is based on three ideal assumptions about you: You have some basic knowledge of matrix algebra. You have taken introductory statistics. You have some experience working with statistical learning techniques. Why this book? As with most of the books that I have written, this book is the result of my professional experience and personal frustration of not having resources like these out there. More specifically, this book was motiviated by the lack of a text that I could refer my students to so they could learn about how various matrix algebra concepts are connected with statistical ideas. Likewise, throughout my teaching activities to different user profiles, I’ve seen time and again that we are not trained to see the connections between many analytical notions, and their matrix-geometric interpretation. This book is my attempt to provide a resource for future (and current) data scientists that may have the software know-how, and a strong understanding of the working principles of the techniques they use, yet they have gaps at the matrix algebra level that prevents them to fully grasp the “under the hood” principles of their daily tools. My hope is that the concepts and notions described in this text will allow you to get a good grasp of the methods discussed in any textbook about Statistical Learning, and how they work. I’m sure this book will be a great resource and consulting text for undergraduate and graduate students, as well as data scientists in the more general sense of this term. Acknowledgements A very special and heartfelt mention to Mohamed Hanafi, who ignited a spark in me that eventually fired up the idea of this work. Very likely I wouldn’t have started writing some notes had he not invited me to be part of his one-man lab in the Unit of Sensometrics and Chemometrics at Oniris-Nantes. I don’t think I’ll ever be able to forget our marathonian discussion sessions, the many dozens of whiteboards filled with illuminating scribbles, the corresponding quota of used markers, the constant skipped lunch breaks, the steady climbing in the volume of his voice as he got excited talking about some abstract concept, and of course, his unique skill for breaking my patience beyond my tolerable limits. Now I know there’s a toll to pay when working with a genious of such astounding talent. I also know I wouldn’t have been able to connect all the dots without him sharing his wisdom, secrets, and utopic dreams. Ustadh Hanafi… Shukran Jazilan. Figure 0.1: Mohamed Hanafi and Gaston Sanchez (Summer 2013) As always, you wouldn’t be reading this book if it wasn’t for the patience and support of my loving wife Jessica. Once again, she allowed me to occupy the dining table as my personal workspace for many months (consequently eating at the couches in countless occasions). If you find any value from the content in this book, which I am sure you will, you owe her almost as much as you owe me. Make a donation If you find this resource useful, please consider making a one-time donation in any amount. Your support really matters. "],
["data.html", "Chapter 1 Data Tables", " Chapter 1 Data Tables Pretty much all statistical learning methods require data in a table format with multiple columns and rows. However, data comes in many different forms. In fact, data can come in many different presentations, and in a wide range of sizes, shapes, colors, textures, and flavors. It can come from one single source, or it may be the result of a variety of sources. It can have a very well defined structure, or it can simply be unstructured. In this chapter, we review some of the common types of data tables that you can typically encounter in practice while applying a statistical learning procedure. "],
["data-individuals-and-variables.html", "1.1 Data: Individuals and Variables", " 1.1 Data: Individuals and Variables To illustrate some of the ideas presented in this chapter I’m going to use a toy example with data of three individuals from a galaxy far, far away: Leia Organa, Luke Skywalker, and Han Solo. Figure 1.1: Three individuals In particular, let’s consider the following information about these subjects: Leia is a woman, 150 centimeters tall, and weighs 49 kilograms. Luke is a man, 172 centimeters tall, and weighs 77 kilograms. Han is a man, 180 centimeters tall, and weighs 80 kilograms. We can say that the above information lists characteristics of three individuals. Or put in an equivalent way, that three subjects are described based on some of their characteristics: Name, Sex, Height, and Weight. Using statistical terminology, we formally say that we have data consisting of three individuals described by some variables. Statistical terminology is abundant and you’ll find that individuals are also known as observations, cases, objects, samples or items. Example of individuals can be people, animals, plants, planets, spaceships, countries, or any other kind of object. A variable is any characteristic of an individual. Sometimes we refer to variables as features, aspects, indicators, descriptors, or properties that we measure or observe on individuals. This combo of individuals and variables is perhaps the most common conceptualization of the term “data” within Statistical Learning methods. But it’s not the only one. Bear in mind that there are other ways in which analysts and researchers think about data. 1.1.1 Representing Data Assuming that our data consists of Name, Sex, Height and Weight for three individuals, we can present this information in various forms. One option is to organize the information in some sort of rectangular or tabular layout, like the one below: Figure 1.2: Conceptual table We can say that this data set is in tabular form, with four columns and three rows (four if you include the row of column names). The same data could be represented in a non-tabular form. An example of a non-tabular format is XML (eXtensible Markup Language). In this case, we can organize the information in a hierarchical way with embeded elements also known as nodes, like in the following example: &lt;character&gt; &lt;name&gt;Leia&lt;/name&gt; &lt;sex&gt;female&lt;/sex&gt; &lt;weight&gt;150 cm&lt;/weight&gt; &lt;height&gt;49 kg&lt;/height&gt; &lt;/character&gt; &lt;character&gt; &lt;name&gt;Luke&lt;/name&gt; &lt;sex&gt;male&lt;/sex&gt; &lt;weight&gt;172 cm&lt;/weight&gt; &lt;height&gt;77 kg&lt;/height&gt; &lt;/character&gt; &lt;character&gt; &lt;name&gt;Han&lt;/name&gt; &lt;sex&gt;male&lt;/sex&gt; &lt;weight&gt;180 cm&lt;/weight&gt; &lt;height&gt;80 kg&lt;/height&gt; &lt;/character&gt; In the above example there are three &lt;character&gt; nodes, each one containing four nodes: &lt;name&gt;, &lt;sex&gt;, &lt;weight&gt;, and &lt;height&gt;. This is an example of what I call raw data. Don’t worry if you are not familiar with XML. I just want to give you an example of the various ways in which data can be organized. From a computational point of view, you could actually find a plethora of formats and conventions used to store information, and data sets in particular. Some formats have been designed to store data in a way that mimics the structure of a rectangular table. But you can find other formats that use a non-tabular structure, like XML. When data is stored in a format that is supposed to represent a table, it is common to find visual displays with some grid of rows and columns. Figure 1.3: Data in tabular format This is another example of raw data, in particular a raw data table. The data is visually organized and displayed in a rectangular grid of cells. However, this table is not ready yet to be manipulated statistically, much less algebraically. Obviously, this table needs some processing and reshaping in order to obtain a table with numerical information that becomes useful for matrix algebra operations. More often than not, raw data will be far from being in tabular format, or at least it will require extra reshaping steps so that it can be ready for the analysis. Although this is a very common issue in practice, in this book I will assume that you’ve already struggled to get the data in the right rectangular shape. 1.1.2 Tabular Data Eventually, to be analyzed with statistical learning tools and other multivariate techniques, your data will typically be required to have a certain specific structure, usually in the form of some sort of table or spreadsheet format (e.g. rows and columns). Figure 1.4: Data Table Format Depending on the nature of the data, and the way it is organized, rows and columns will have a particular meaning. The most common data table setting is the one of individuals-and-variables. Although somewhat arbitrary, the standard convention is that we use the columns to represent variables, and the rows to represent individuals. Figure 1.5: Conceptual table of individuals and variables "],
["types-of-tables.html", "1.2 Types of Tables", " 1.2 Types of Tables The most typical table format is that of individuals (rows) and variables (columns). However, the individuals-variables layout is not the only type of setting; there are other types of tables like contingency tables, crosstabulations, distance tables, as well as similarity and proximity tables. So let’s review some examples of various kinds of rectangular formats. 1.2.1 Heterogeneous Table Perhaps the most ubiquitous type of table is that of inidividuals and variables in which the variables represent mixed or heterogeneous information. The toy data introduced so far is an example of a heterogeneous table involving distinct flavors of variables such as Name, Sex, Hieght and Weight. In other words, Name and Sex have strings values or categories, while Height and Weight have numeric values (representing quantities). Figure 1.6: Mixed or heterogeneous variables As you can tell, Height and Weight are already expressed in numeric values, and you can actually do some math on them (i.e. apply arithmetic and algebraic operations). In contrast, Name and Sex are not codified numerically, so the type of mathematical operations that you can perform on them is very limited. In or der to exploit their information in a deeper sense, you would have to transform the categories male and female with some numeric coding. 1.2.2 Binary table Another common type of table is a binary table. As its name indicates, this type of table contains variables that can only take two values. For example presence-absence, female-male, yes-no, success-failure, case-control, etc. In the table below, the variables represent drinks consumed: Beer, Wine, Juice, Coffee, and Tea. Each variable takes two possible values, yes and no, indicating whether an individual consumes a specific type of drink. Figure 1.7: Binary table (raw values) Although the values yes and no are very descriptive, you will need to codify them numerically to be able to perform statitical or algebraic operations with them. Perhaps the most natural way to codify binary values is with zeros and ones: “yes” = 1, and “no” = 0. Figure 1.8: Binary table (numeric values) Another possible codification could be “yes” = 1, and “no” = -1. Or also with logical values: “yes” = TRUE, and “no” = FALSE. 1.2.3 Modalities table Another type of table consists of so-called modalities. These can come from variables or questions in a survey about how frequent you use/consume a specific product. Figure 1.9: Table of modalities (raw values) In order to statistically treat a modalities table, you will very likely have to transform the values of the categories (i.e. the modalities) to some numeric coding. For instance, you can assign values 1 = “never”, 2 = “sometimes”, and 3 = “always.” Figure 1.10: Table of modalities (numeric values) 1.2.4 Preference table A preference table is a special case of individuals-variables table in which the variables are measured in some kind of preference scale. For example, we can measure the preference level for various types of fruit juices on an ordinal scale ranging from 1 = “don’t like at all” to 5 = “like it very much.” Figure 1.11: Table of frequencies 1.2.5 Frequency table As its name suggests, this type of table contains the frequencies (i.e. counts) resulting from crossing two categorical variables. This is the reason why you also find the name cross-tables for this type of tabular data. Another common name for this type of tables is contingency table. The example below shows a frequency table of the number of dialogues of each character, per episode (in the original trilogy of Star Wars). The rows correspond to the categories of the variable Name, while the columns corresponds to the categories of the variable Episode. Figure 1.12: Table of frequencies The value in the ij-th cell (i-th row, j-th column) tells you the number of occurrences that share Name’s category i and Episode’s category j. If you add all the entries, you get the total number of individuals in each variable. This tabular format is not really an individuals-variables table. Even though the example above has rows with names of the three individuals, the way you get a table like this is with two categorical variables. 1.2.6 Distance table Another interesting type of table is a distance table. Depending on who you talk to, the term “distance” may be used with slightly different meanings. Some authors refer to the word distance conveying a metric distance meaning. Other authors instead use the word distance to convey a general idea of dissimilarity. In general, you can find distance tables under two opposite perspectives: similarities and disimilarities. The table below is an example of a similarity table. Figure 1.13: Table of proximities 1.2.7 Summary Most statistical learning methods require data in a table format with multiple columns and rows. It’s important to be aware of the difference between a raw data table, and a clean table numerically codified. Unless otherwise specified, in this book we’ll assume that all the variables in a data matrix have numerical variables. For most of this book, we’ll consider a data set to be integrated by a group of individuals or objects on which we observe or measure one or more characteristics called variables. Make a donation If you find this resource useful, please consider making a one-time donation in any amount. Your support really matters. "],
["datamatrix.html", "Chapter 2 Data Matrix", " Chapter 2 Data Matrix In the previous chapter we talked about the standard notion of data from the statistical learning perspective, that is, as a “set of individuals described by one or more variables”. In addition, we looked at various types of data tables, and we also discussed about the distinction between raw tables and clean tables. In this chapter, we make the transition from a data table to a data matrix. I will introduce some notation used in the rest of the book, and focus on how to think of a data table in terms of a matrix, and what aspects to keep in mind while converting any data table into a mathematical matrix. "],
["about-matrices.html", "2.1 About Matrices", " 2.1 About Matrices First let’s talk about matrices in the mathematical sense of the word. A matrix is a rectangular array of elements. More formally, a matrix is a mathematical object whose structure is a rectangular array of entries. Typically, we consider the elements of a matrix to be numbers. Although you can certainly generalize the contents of a matrix and think about them as any type of symbols, not necessarily numbers. This is perhaps a common way to conceive a generic matrix from a more computational-oriented perspective. From example, an R matrix may be numeric, but you can also have matrices containing logical values (TRUE or FALSE), or even a matrix made up of characters. # numeric matrix A &lt;- matrix(1:12, nrow = 4, ncol = 3) A [,1] [,2] [,3] [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 # logical matrix L &lt;- matrix(c(TRUE, FALSE), nrow = 4, ncol = 3) L [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] FALSE FALSE FALSE [3,] TRUE TRUE TRUE [4,] FALSE FALSE FALSE # character matrix S &lt;- matrix(sample(letters, 12), nrow = 4, ncol = 3) S [,1] [,2] [,3] [1,] &quot;c&quot; &quot;e&quot; &quot;o&quot; [2,] &quot;v&quot; &quot;k&quot; &quot;q&quot; [3,] &quot;w&quot; &quot;z&quot; &quot;u&quot; [4,] &quot;b&quot; &quot;h&quot; &quot;i&quot; "],
["data-matrix.html", "2.2 Data Matrix", " 2.2 Data Matrix While the raw ingredient for any statistical learning endevour is a data set, typically reshaped and organized in a tabular form, the core input of any statistical learning method consists of the data matrix, or in some cases, matrices. To make things less abstract I’m going to use the following toy data table with four variables measured on six individuals: gender height weight jedi Anakin Skywalker male 1.88 84.0 yes_jedi Padme Amidala female 1.65 45.0 no_jedi Luke Skywalker male 1.72 77.0 yes_jedi Leia Organa female 1.50 49.0 no_jedi Qui-Gon Jinn male 1.93 88.5 yes_jedi Obi-Wan Kenobi male 1.82 77.0 yes_jedi You can actually find the corresponding CSV file in the data/ folder of the book’s github repository. https://github.com/gastonstat/matrix4sl/blob/master/data/starwars.csv 2.2.1 Why a data matrix? The reason to use matrices is simple: the typical structure in which the data can be mathematically manipulated from a multivariate standpoint is a data matrix. No other mathematical object adapts so well to the rectangular structure of a data table. Since we are getting into more formal mathematical territory, we need to define some terminology and notation that we’ll use in the rest of the book. We are going to represent matrices with bold upper-case letters like \\(\\mathbf{A}\\) or \\(\\mathbf{B}\\). In general, a matrix \\(\\mathbf{X}\\) has \\(n\\) rows and \\(p\\) columns, and we say that \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix. In addition, we are going to assume that all the matrices are real matrices, that is, matrices containing elements in the set of Real numbers. \\[ \\underset{n \\times p}{\\mathbf{X}} = \\left[\\begin{array}{ccccc} x_{11} &amp; \\cdots &amp; x_{1j} &amp; \\cdots &amp; x_{1p} \\\\ \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{i1} &amp; \\cdots &amp; x_{ij} &amp; \\cdots &amp; x_{ip} \\\\ \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{n1} &amp; \\cdots &amp; x_{nj} &amp; \\cdots &amp; x_{np} \\\\ \\end{array}\\right] \\] An element of a matrix \\(\\mathbf{X}\\) is denoted by \\(x_{ij}\\) which represents the element in the \\(i\\)-th row and the \\(j\\)-th column. For instance, the element \\(x_{23}\\) in our toy \\(6 \\times 4\\) data matrix has the value 45 and it represents the weight of Padme Amidala. Together with matrices we also have variables, and vectors. We are going to represent variables with italized capital letters such as \\(Y\\) and \\(Z\\). Also, variables in a matrix \\(\\mathbf{X}\\) can be represented with subscripts such as \\(X_j\\) to indicate the column-index position. The first three variables in our matrix \\(\\mathbf{X}\\) would be represented as \\(X_1\\) (gender), \\(X_2\\) (height), and \\(X_3\\) (weight). Vectors will be represented by bold lower case letters such as \\(\\mathbf{w}\\) or \\(\\mathbf{u}\\). A vector \\(\\mathbf{w}\\) with \\(m\\) elements can be expressed as \\(\\mathbf{w} = (w_1, w_2, \\dots, w_m)\\). Since variables can be expressed as vectors, we can use the vector notation \\(\\mathbf{x}\\) to represent a variable \\(X\\). However, keep in mind that not all vectors are variables: a vector may well refer to an object or to any row of a matrix. 2.2.2 Matrix Representations You will find that people represent matrices in multiple ways, using various notations, symbols, and diagrams. We can represent a data matrix \\(\\mathbf{X}\\) by simply regarding the matrix from its columns (i.e. variables) perspective: \\[ \\mathbf{X} = \\left[\\begin{array}{c|c|c|c|c|c} &amp; &amp; &amp; &amp; &amp; \\\\ \\mathbf{x}_{1} &amp; \\mathbf{x}_{2} &amp; \\cdots &amp; \\mathbf{x}_{j} &amp; \\cdots &amp; \\mathbf{x}_{p} \\\\ &amp; &amp; &amp; &amp; &amp; \\\\ \\end{array}\\right] \\] Note that each variable \\(X_j\\) is represented with a column vector \\(\\mathbf{x}_{j}\\). In turn, the vertical lines are just a visual cue indicating separation between columns. Taking our toy data data matrix, we could represent it as: \\[ \\mathbf{Data} = \\left[\\begin{array}{c|c|c|c} &amp; &amp; &amp; \\\\ \\texttt{gender} &amp; \\texttt{height} &amp; \\texttt{weight} &amp; \\texttt{jedi} \\\\ &amp; &amp; &amp; \\\\ \\end{array}\\right] \\] Being more minimalists, we can even get a more compact representation by just expressing a matrix \\(\\mathbf{X}\\) as a list or sequence of its variables: \\[ \\mathbf{X} = \\left[\\begin{array}{cccccc} \\mathbf{x}_{1} &amp; \\mathbf{x}_{2} &amp; \\cdots &amp; \\mathbf{x}_{j} &amp; \\cdots &amp; \\mathbf{x}_{p} \\\\ \\end{array}\\right] \\] 2.2.3 A Block of Variables While most of us are used to think of multivariate data in terms of a table or matrix, there can be other interesting representation options. If you are like me who loves to visualize concepts, we can also use an alternative way to display the variables using a standard convention of drawing them in rectangular shape. In this way, we can depict a set of variables in a data matrix as a series of rectangles, like those below inside the dashed box: Figure 2.1: Variables in a matrix depicted as a set of rectangles The rectangular-box shape perfectly illustrates the idea of a block of variables. I will use the term block to convey the idea of a group or set of variables measured on a set of objects. If you prefer, you can also think of a block as a data table. "],
["types-of-matrices.html", "2.3 Types of Matrices", " 2.3 Types of Matrices There are various kinds of matrices which are commonly used in statistical learning methods. Let’s briefly revisit some of them. 2.3.1 General Rectangular Matrix The most general type of matrix is a rectangular matrix. This means that there is an arbitrary number of \\(n\\) rows and \\(p\\) columns. \\[ \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\\\ 10 &amp; 11 &amp; 12 \\\\ \\end{array}\\right] \\] If a matrix has more rows than columns \\(n &gt; p\\), sometimes we call this a vertical or tall matrix: \\[ \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\\\ 10 &amp; 11 &amp; 12 \\\\ 13 &amp; 14 &amp; 15 \\\\ \\end{array}\\right] \\] In contrast, if a matrix has more columns than rows, \\(p &gt; n\\), sometimes we call this a horizontal, flat or wide matrix: \\[ \\left[\\begin{array}{cccc} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\ 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 \\\\ \\end{array}\\right] \\] 2.3.2 Square Matrix A square matrix is one with same number of rows and columns, \\(n = p\\). In statiscal learning, various matrices derived from a data matrix are typically square, this usually happens when working with derived cross-products such as \\(\\mathbf{X^\\mathsf{T} X}\\) or \\(\\mathbf{X X^\\mathsf{T}}\\). \\[ \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\\\ \\end{array}\\right] \\] 2.3.3 Symmetric Matrix A symmetric matrix is a special case of a square matrix: the corresponding elements of corresponding rows and columns are equal. In other words, in a symmetric matrix we have that the element \\(x_{ij}\\) is equal to \\(x_{ji}\\). \\[ \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\\\ \\end{array}\\right] \\] 2.3.4 Diagonal Matrix A diagonal matrix is a special case of symmetric matrix in which all nondiagonal elements are 0. \\[ \\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\\\ \\end{array}\\right] \\] In other words, in a symmetric matrix, \\(x_{ij} = 0\\) for \\(i \\neq j\\). The only elements that may not necessarily be qual to zero are those in the diagonal. \\[ \\left[\\begin{array}{ccc} x_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; x_{22} &amp; 0 \\\\ 0 &amp; 0 &amp; x_{33} \\\\ \\end{array}\\right] \\] 2.3.5 Scalar Matrix The scalar matrix is a special type of diagonal matrix in which all the diagonal elements are equal scalars. \\[ \\left[\\begin{array}{ccc} 5 &amp; 0 &amp; 0 \\\\ 0 &amp; 5 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 \\\\ \\end{array}\\right] \\] 2.3.6 Identity Matrix The identity matrix is a special case of the scalar matrix in which the diagonal elements are all equal to one. This matrix is typically represented with the letter \\(\\mathbf{I}\\). To indicate the dimensions of this matrix, we typically write it with a subscript for the number of rows (or columns). For example, \\(\\mathbf{I}_{n}\\) represents the identity matrix of dimension \\(n \\times n\\). \\[ \\mathbf{I}_{3} = \\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right] \\] In matrix algebra, the identity matrix plays the same role as the number 1 in ordinary or scalar algebra. 2.3.7 Triangular Matrix A square matrix with only 0’s above or below the diagonal is called a triangular matrix. There are two flavors of this type of matrix: upper triangular and lower triangular. An upper triangular matrix has zeros below the diagonal. \\[ \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 5 &amp; 6 \\\\ 0 &amp; 0 &amp; 9 \\\\ \\end{array}\\right] \\quad \\textsf{or} \\quad \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ &amp; 5 &amp; 6 \\\\ &amp; &amp; 9 \\\\ \\end{array}\\right] \\] Conversely, a lower triangular matrix has zeros above the diagonal. \\[ \\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 4 &amp; 5 &amp; 0 \\\\ 7 &amp; 8 &amp; 9 \\\\ \\end{array}\\right] \\quad \\textsf{or} \\quad \\left[\\begin{array}{ccc} 1 &amp; &amp; \\\\ 4 &amp; 5 &amp; \\\\ 7 &amp; 8 &amp; 9 \\\\ \\end{array}\\right] \\] 2.3.8 Shapes of Matrices To summarize the various types of matrices described so far, here’s a diagram depicting their shapes. The general type of matrix is a rectangular matrix which can be divided in three types: vertical or tall (\\(n &gt; p\\)), square (\\(n = p\\)), and horizontal or wide (\\(n &lt; p\\)). Among square matrices, the notion of symmetry plays a special role, and among symmetric matrices, we can say that diagonal ones have the most basic structure. Figure 2.2: Various shapes of matrices Make a donation If you find this resource useful, please consider making a one-time donation in any amount. Your support really matters. "],
["variables.html", "Chapter 3 Variables", " Chapter 3 Variables In statistical learning, the most typical data format involves a set of individuals or objects described by several characteristics commonly known as variables. Likewise, the most common format in which data is organized is in a table with columns representing variables, and rows representing individuals, like in the following figure. Figure 3.1: Conceptual table of individuals and variables Before moving onto more matrix algebra concepts, we need to discuss a couple of things about variables in terms of how they can be classified, and also how users tend to encode values in them. "],
["types-of-variables.html", "3.1 Types of Variables", " 3.1 Types of Variables To illustrate some of the ideas presented in this chapter I’m going to use a toy example with data from the characters of the Star Wars universe. You can actually find the corresponding CSV file in the data/ folder of the book’s github repository. name gender height weight species jedi weapon 1 Anakin Skywalker male 1.88 84.0 human yes_jedi lightsaber 2 Padme Amidala female 1.65 45.0 human no_jedi unarmed 3 Luke Skywalker male 1.72 77.0 human yes_jedi lightsaber 4 Leia Organa female 1.50 49.0 human no_jedi blaster 5 Qui-Gon Jinn male 1.93 88.5 human yes_jedi lightsaber 6 Obi-Wan Kenobi male 1.82 77.0 human yes_jedi lightsaber 7 Han Solo male 1.80 80.0 human no_jedi blaster 8 Sheev Palpatine male 1.73 75.0 human no_jedi force-lightning 9 R2-D2 male 0.96 32.0 droid no_jedi unarmed 10 C-3PO male 1.67 75.0 droid no_jedi unarmed 11 Yoda male 0.66 17.0 yoda yes_jedi lightsaber 12 Darth Maul male 1.75 80.0 dathomirian no_jedi lightsaber 13 Dooku male 1.93 86.0 human yes_jedi lightsaber 14 Chewbacca male 2.28 112.0 wookiee no_jedi bowcaster 15 Jabba male 3.90 NA hutt no_jedi unarmed 16 Lando Calrissian male 1.78 79.0 human no_jedi blaster 17 Boba Fett male 1.83 78.0 human no_jedi blaster 18 Jango Fett male 1.83 79.0 human no_jedi blaster 19 Grievous male 2.16 159.0 kaleesh no_jedi slugthrower 20 Chief Chirpa male 1.00 50.0 ewok no_jedi spear The table consists of 20 rows and 7 columns. The rows correspond to individuals and the columns correspond to variables. Although this data set is a toy example, it contains variables of different types commonly found in real data sets. Interestingly, we can classify variables in a couple of different ways. The most basic and usual way to classify variables is in two distinct types: quantitative variables and categorical (or qualitative) variables. The variables height and weight are examples of quantitative variables because their values represent quantities. That is, they can be measured numerically on some sort of interval scale. In turn, variables such as name, gender, species, jedi, and weapon are categorical or qualitative variables because their values represent categories (or qualities). More formally, they describe a quality of an individual, and allows you to place an individual into a category or group, such as male or female. The division between categorical and quantitative variables is not the only one. Often, data scientists further classifiy categorical variables as nominal or ordinal. Likewise, quantitative variables can be classified as discrete or continuous. This next level of classification is chiefly based on the notion of scales of measurement of the variables. Figure 3.2: Further classification of variables 3.1.1 Nominal Variable A categorical variable is nominal when it results from naming or labeling values that don’t have a natural order. An example of a nominal variable is weapon which has the following values: [1] &quot;blaster&quot; &quot;bowcaster&quot; &quot;force-lightning&quot; &quot;lightsaber&quot; [5] &quot;slugthrower&quot; &quot;spear&quot; &quot;unarmed&quot; Can you order the categories in a “natural” way? Not really. The term nominal according the dictionary means “existing in name only”. Thus, nominal values are just that: names. There is no reason why blaster is better or greater than lightsaber. You could say that you prefer a blaster over a lightsaber but that’s a different variable: personal preference. Other typical examples of nominal variables are: the sex of a newborn child: e.g. female or male the ethnicity of an individual: e.g. Native-American, African-American, Asian, White ice cream flavors: e.g. chocolate, vanilla, strawberry the numbers on the players’ jerseys of a soccer team: numbers used as identifiers 3.1.2 Ordinal Variable A categorical variable is ordinal when it results from ordering values into a series of categories when no appropriate numerical scale is available. For example, consider a variable “usage frequency” measured with values never, sometimes, and always. In this case we can order the categories from less usage to more usage, or viceversa. Some examples of ordinal variables are: size of clothes: extra-small, small, medium, large, extra-large college year: freshman, sophomore, junior, senior spiciness: none, mild, moderate, very jedis ranks: youngling, padawan, knight, master, and grand master 3.1.3 Discrete Variable A quantitative variable is discrete when it results from counting. To be more precise, a discrete variable takes on zero or a positive integer value. Some examples of discrete variables are: the number of male ewooks in a family with four children (0, 1, 2, 3, or 4). the number of robots per Imperial Star Destroyer the number of moons orbiting around a planet 3.1.4 Continuous Variable A quantitative variable is continuous when it results from measuring. More technically, a continuous variable theoretically takes on an infinite number of possible values, however, its reported values are subject to the precision or accuracy of the measurement device. Some examples of continuous variables are: the height of an individual the weight of a robot the speed of a starship 3.1.5 Caveat Keep in mind that not all variables fit neatly and unambiguously into one of the previous classes. For example, the age of an individual could be considered of a discrete variable when it gets reported in (whole) number of years. However, age could also be considered to be continuous when measured in a more granular scale: e.g. days, or hours, or seconds. Moreover, sometimes age is reported into ordered categories such as 0 to 5 years, 6 to 10, 11 to 15, and so on. These values would turn age into an ordinal variable. "],
["encoding-values.html", "3.2 Encoding Values", " 3.2 Encoding Values Another important aspect intimately connected with the various types of variables is that of how variable values are encoded. To better understand what this is about, let me discuss a simple example. Notice that the quantitative variables in the star wars data set have numeric values, while the categorical variables have non-numeric values. Does that mean that all numeric variables can be considered to be quantitative? And that all non-numeric variables can be considered to be categorical? The answer is: Not necessarily. Often, analysts assign numeric values to the categories of a qualitative variable. Sooner or later you will find variables with numeric values that are not quantitative. An example could be ice-cream flavors in which the categories are codified with numbers. For example, consider three ice-cream flavors: vanilla, chocolate, and lemon. And imagine that we assign a numeric code to each flavor: 1 = vanilla, 2 = chocolate, 3 = lemon. This numeric labeling is just for convenience purposes. Assume that you get the preferred ice-cream flavor of 10 subjects with the following values: 3 2 3 2 1 3 3 3 3 3 The above numbers, which are hypothetical values of a variable icecream, do not represent quantities; they represent flavors. Just because there are numbers, it does not mean that we can use those numbers to carry out arithmetic operations. What is the result of: 3 - 1, that is, lemon minus vanilla? It is meaningless to attempt this type of operation. Likewise, we could assign numbers to sizes: 1=small, 2=medium, 3=large. In this case, the numbers are again used for convenient purposes. And we can even take one further step and say that we can use the numbers to rank the categories. But it will be impossible to add 1+2, since small + medium does not equal large. The point is that just because a variable contains numbers, that doesn’t automatically make it quantitative. You should always ask yourself if the numbers represent some sort of quantity. If the answer is a sounded yes, then you have a quantitative variable. Otherwise, you have a categorical one. 3.2.1 More on encoding We can find categorical data under a wide range of formats. I’ve seen categorical data codified in different ways, and sometimes people are very creative in the way they do this. The main types of formats can be classified in three main groups: text or characters numbers (ideally integers) logical (TRUE / FALSE), typically for binary variables Here’s an example with a gender variable: as text: \"F\" (female), \"M\" (male) as numbers: 1 (female), 0 (male) as logical: TRUE (female), FALSE (male) When talking about the way data is stored and encoded, I don’t think there’s an ideal/universal way to store categorical data effectively and efficiently. It all depends on the field of application, the size of the data, the legibility, the usage purposes, etc. What I do believe in is that, when categorical data is being analyzed, we should consider a couple of issues: understandability: the analyst should be able to read, interpret and understand what the available values represent. Whenever possible, you should aim to reduce friction, and avoid having to struggle with decodifying numbers. compatibility: this has to do with functions and commands in data analysis and statistical software. Some functions are programmed in a way that they accept a specific type of input (either a vector, a factor, a data frame, etc). Often, you will need to manipulate some kind of object in order to convert it into another type of object better suited for a certain functions or command. visibility: this aspect is related to visual displays in graphics. Maybe long labels look fine in a table, but for plotting purposes they could cluttered the screen. Make a donation If you find this resource useful, please consider making a one-time donation in any amount. Your support really matters. "],
["transformations.html", "Chapter 4 Transformations", " Chapter 4 Transformations In many situations you will need to transform and modify the raw values of your data. Transformations are useful for many reasons, for example: to change the scale of a variable from continuous to discrete to linearize a variable that has a non-linear distribution to make distributions more symmetric to re-center the data (mean-center, or center by other reference value) to stretch or compress the values (by standard deviation, by range, by eigenvalues) to binarize or dummify a categorical variable In this chapter, I will cover common transformations: dummyfication mean-center standardization logarithmic transformation power transformation "],
["dummy-variables.html", "4.1 Dummy Variables", " 4.1 Dummy Variables As we saw in the previous chapter, categorical variables are characteristics or qualities observed on individuals. An example of a categorical variable is Sex with possible values male and female. Most of the time, categorical variables will be codified in a non-numeric way, usually as strings or character values. However, it is not unusual to find numeric codes such as “female” = 1, and “male” = 2. More often than not, it is convenient to code categorical variables as dummy variables, that is, decompose a categorical variable into one or more indicator variables that take on the values 0 or 1. For example, suppose that Sex has two values male and female. This variable can be coded as two dummy variables which have values [1 0 0] for female, [0 1 1] for male: \\[ \\left[\\begin{array}{c} female \\\\ male \\\\ male \\\\ \\end{array}\\right] = \\left[\\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{array}\\right] \\] For some statistical learning methods such as regression analysis—due to a technical reason—only one of these two dummies can be used. This means that you would drop one of the dummy columns: \\[ \\left[\\begin{array}{c} female \\\\ male \\\\ male \\\\ \\end{array}\\right] = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ \\end{array}\\right] \\] The resulting one dummy variable is interpreted as the difference of female and male, keeping female as the reference value. "],
["standardization.html", "4.2 Standardization", " 4.2 Standardization When variables are quantitative, they can be measured in the same scale. But they can also be measured in different scales. For example, consider four variables: 1) Weight measured in kilograms, 2) Height measured in centimeters, 3) Income measured in dollars, and 4) Temperature measured in Celsius degrees. When you use a method in which comparisons or calculations are made taking into account the variance of the variables, you will face an interesting phenomenon: the variable that has the largest magnitude will dominate the variability in the data. And this could be a (serious) problem. To compensate for the differences in scales, something must be done. The question is then: how to balance the contributions of the variables in a way that you can have a fair comparison among them? The key is to put them all under a common scale. Let’s review four options to standardize a variable: by standard deviations from the mean (i.e. standard units) by the overall range by chosen percentiles by the mean As a working example, let’s use the same data from chapter Data Matrix. gender height weight jedi Anakin Skywalker male 1.88 84.0 yes_jedi Padme Amidala female 1.65 45.0 no_jedi Luke Skywalker male 1.72 77.0 yes_jedi Leia Organa female 1.50 49.0 no_jedi Qui-Gon Jinn male 1.93 88.5 yes_jedi Obi-Wan Kenobi male 1.82 77.0 yes_jedi We will use variable height and denoted with \\(X\\): x &lt;- dat$height x [1] 1.88 1.65 1.72 1.50 1.93 1.82 4.2.1 By Standard Units One way to obtain a common scale is to standardize the variables by number of standard deviations from the mean. The goal is to convert a variable \\(X\\) into a variable \\(Z\\) in standard units. You do this by subtracting the mean \\(\\bar{x}\\) from every value \\(x_i\\), and then divide by the standard deviation \\(s\\). The conversion formula is: \\[ z_i = \\frac{x_i - \\bar{x}}{s} \\] where \\(\\bar{x}\\) is the mean of \\(X\\), and \\(s\\) is the standard deviation of \\(X\\). A value that is expressed in standard units measures how far that value is from the mean, in terms of standard deviations. In vector notation, the standardized vector \\(\\mathbf{z}\\) of \\(\\mathbf{x}\\) is: \\[ \\mathbf{z} = \\frac{1}{s} (\\mathbf{x} - \\mathbf{\\bar{x}}) \\] Here’s some code in R that shows step-by-step the operations to obtain a variable in standard units: # mean x_mean &lt;- mean(x) x_mean [1] 1.75 # standard deviation x_sd &lt;- sd(x) x_sd [1] 0.16 # height in stadard units std_units &lt;- (x - x_mean) / x_sd std_units [1] 0.814 -0.626 -0.188 -1.565 1.127 0.438 The standardized vector std_units now has a mean of 0 and standard deviation 0: mean(std_units) [1] -2.73e-16 sd(std_units) [1] 1 4.2.2 By the Overall Range Another kind of standardization is to subtract the minimum value from every value \\(x_i\\), and then divide by the overall range: \\(max(x) - min(x)\\). \\[ z_i = \\frac{x_i - min(x)}{max(x) - min(x)} \\] Notice that this type of standardization will produce a variable \\(Z\\) that is linearly transformed ranging from 0 to 1, where 0 is its minimum and 1 its maximum value. # maximum x_max &lt;- max(x) x_max [1] 1.93 # minumum x_min &lt;- min(x) x_min [1] 1.5 # height standardized by overall range std_range &lt;- (x - x_min) / (x_max - x_min) std_range [1] 0.884 0.349 0.512 0.000 1.000 0.744 The standardized vector std_range now ranges from 0 to 1: min(std_range) [1] 0 max(std_range) [1] 1 4.2.3 By Chosen Percentiles A general type of range-based standardization is to divide by a different type of range, not just the overall range. This is done by using any range given from a pair of symmetric percentiles. For example, you can choose the 25-th and the 75-th percentiles, also known as the first quartile \\(Q_1\\) and third quartile \\(Q_3\\), respectively. In other words, standardized by the interquartile range or IQR: \\[ z_i = \\frac{x_i}{Q_3 - Q_1} = \\frac{x_i}{\\text{IQR}(x)} \\] Why to standardize by chosen percentiles other than the maximum and the minimum? Because the overall range is sensitive to outliers; so to have a less sensitive scale, we divide by a more robust range. As another example of this type of standardization, you can divide by the range between the 5-th and the 95-th percentiles. # inter-quartile range x_iqr &lt;- IQR(x) # height standardized by IQR std_iqr &lt;- x / x_iqr std_iqr [1] 9.52 8.35 8.71 7.59 9.77 9.22 The standardized vector std_iqr now has an IQR of 1: IQR(std_iqr) [1] 1 4.2.4 By the Mean A less common but equally interesting type of standardization is to divide the values \\(x_i\\) by their mean \\(\\bar{x}\\) which causes transformed values \\(z_i\\) to have standard deviations equal to their coefficient of variation: \\[ z_i = \\frac{x_i}{\\bar{x}} \\] 4.2.5 About standardization All the previous standardizations can be put in terms of a form of weighting given by the following formula: \\[ z_i = w \\times x_i \\] where \\(w\\) represents a weight or scaling factor that can be: the standard deviation: \\(w = s\\) the overall range: \\(w = max(x) - min(x)\\) the IQR: \\(w = \\text{IQR}\\) the mean: \\(w = \\bar{x}\\) "],
["logarithmic-transformations.html", "4.3 Logarithmic Transformations", " 4.3 Logarithmic Transformations It is not uncommon to find ratio variables with distributions that are skew. While this is not necessarily an issue, certain statistical learning methods assume that variables have fairly symmetric bell-shaped distributions. One transformation typically used by analysts to obtain more symmetric distributions from skewed ones is the logarithmic one. When a variable has positive values, we can log-transform it. This will shrink or compact the long tails, making them more symmetrical. In addition, it will also convert multiplicative relationships to additive ones. Why? Because of the properties of logarithm: \\(log(xy) = log(x) + log(y)\\) and \\(log(x/y) = log(x) - log(y)\\). "],
["power-transformations.html", "4.4 Power Transformations", " 4.4 Power Transformations A transformation that is similar to the logarithmic one is the Box-Cox transformation. \\[ z = \\frac{1}{\\lambda} (x^\\lambda - 1) \\] where \\(\\lambda\\) is a positive real number that plays the role of a power parameter. Interestingly, the Box-Cox transformation approximates to the log-transformation as the power parameter \\(\\lambda\\) tends to 0. The division by \\(\\lambda\\) has a reason: it keeps the scale of the original variable from collapsing. For instance, if you take the 10th roots \\(x^{0.1}\\) of a variable, you should see that all the values are close to 1, so dividing by 1/10 multiplies the values by 10, which is almost a logarithmic scale. In summary, Box-Cox transformations provide a way of making data more symmetric, which could be very helpful in regression analysis tools. Make a donation If you find this resource useful, please consider making a one-time donation in any amount. Your support really matters. "],
["duality.html", "Chapter 5 Geometric Duality", " Chapter 5 Geometric Duality In chapter Data Matrix we talked about how a data table can be mathematically treated as a data matrix: typically as an array of individuals and variables. In this chapter we take a further step that should let you adopt a geometrical mindset. More specifically, you will learn how to think and look at any data matrix from a geometric standpoint. This is an incredible insightful concept which some authors refer to as the duality of a data matrix. "],
["matrix-perspectives.html", "5.1 Matrix Perspectives", " 5.1 Matrix Perspectives It’s very enlightening to think of a data matrix as viewed from the glass of Geometry. The key idea is to think of the data in a matrix as elements living in a multidimensional space. Actually, we can regard a data matrix from two apparently different perspectives that, in reality, are intimately connected: the rows perspective and the columns perspective. In order to explain these perspectives, let me use the following diagram of a data matrix \\(\\mathbf{X}\\) with \\(n\\) rows and \\(p\\) columns, with \\(x_{ij}\\) representing the element in the \\(i\\)-th row and \\(j\\)-th column. Figure 5.1: Duality of a data matrix When we look at a data matrix from the columns perpective what we are doing is focusing on the \\(p\\) variables. In a similar way, when looking at a data matrix from its rows perspective, we are focusing on the \\(n\\) individuals. This double perspective or duality for short, is like the two sides of the same coin. 5.1.1 Rows Space We know that human vision is limited to three-dimensions, but pretend that you had superpowers that let you visualize a space with any number of dimensions. Because each row of the data matrix has \\(p\\) elements, we can regard individuals as objects that live in a \\(p\\)-dimensional space. For visualization purposes, think of each variable as playing the role of a dimension associated to a given axis in this space; likewise, consider each of the \\(n\\) individuals as being depicted as a point (or particle) in such space, like in the following diagram: Figure 5.2: Rows space In the figure above, even though I’m showing only three axes, you should pretend that you are visualizing a \\(p\\)-dimensional space (imaging that there are \\(p\\) axes). Each point in this space corresponds to a single individual, and they all form what you can call a cloud of points. 5.1.2 Columns Space We can do the same visual exercise with the columns of a data matrix. Since each variable has \\(n\\) elements, we can regard the set of \\(p\\) variables as objects that live in an \\(n\\)-dimensional space. For convenient purposes, and in order to distinguish them from the individuals, we use an arrow (or vector) to graphically represent each variable: Figure 5.3: Columns space Analogously to the rows space and its cloud of individuals, you should also pretend that the image above is displaying an \\(n\\)-dimensional space with a bunch of blue arrows pointing in various directions. 5.1.3 Toy Example To make things less abstract, let’s consider a toy \\(2 \\times 2\\) data matrix of weight and height values measured on two individuals: Leia and Luke. Here’s the code in R to create a matrix X for this example: # toy matrix X &lt;- matrix(c(150, 172, 49, 77), nrow = 2, ncol = 2) rownames(X) &lt;- c(&quot;Leia&quot;, &quot;Luke&quot;) colnames(X) &lt;- c(&quot;weight&quot;, &quot;height&quot;) X weight height Leia 150 49 Luke 172 77 Space of individuals: If you look at the data in X from the individuals standpoint, each of them can be depicted as a dot in the 2-dimensional space spanned by variables weight and height: Space of variables: If you look at the data in X from the variables standpoint, each of them can be depicted as a vector in the 2-dimensional space spanned by Leia and Luke: 5.1.4 Dual Space The following figure illustrates the dual perspective of a data matrix. The standard convention is to represent individuals as points in a \\(p\\)-dimensional space; and in turn represent variables as vectors in an \\(n\\)-dimensional space. Figure 5.4: Rows and columns perspectives "],
["vector.html", "Chapter 6 Vector Basics", " Chapter 6 Vector Basics Because variables can be treated as vectors in a geometric sense, I would like to briefly revisit a couple of key concepts for operating with vectors: the inner product and derived operations. We’ll keep using the toy \\(2 \\times 2\\) data matrix of weight and height discussed in the previous chapter. Here’s the code in R for such matrix: # data matrix X &lt;- matrix(c(150, 172, 180, 49, 77, 80), nrow = 3, ncol = 2) rownames(X) &lt;- c(&quot;Leia&quot;, &quot;Luke&quot;, &quot;Han&quot;) colnames(X) &lt;- c(&quot;weight&quot;, &quot;height&quot;) X weight height Leia 150 49 Luke 172 77 Han 180 80 "],
["inner-product.html", "6.1 Inner Product", " 6.1 Inner Product The concept of an inner product is one of the most important matrix algebra concepts, also commonly referred to as the dot product. The inner product is a special operation defined on two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) that, as its name indicates, allows us to multiply \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) in a certain way. The inner product of two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)—of the same size— is defined as: \\[ \\mathbf{x \\cdot y} = \\sum_{i = 1}^{n} x_i y_i \\] basically the inner product consists of the element-by-element product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), and then adding everything up. The result is not another vector but a single number, a scalar. We can also write the inner product \\(\\mathbf{x \\cdot y}\\) in vector notation as \\(\\mathbf{x^\\mathsf{T} y}\\) since \\[ \\mathbf{x^\\mathsf{T} y} = (x_1 \\dots x_n) \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} = \\sum_{i = 1}^{n} x_i y_i \\] Consider the data about Leia and Luke used in the last chapter: weight height Leia 150 49 Luke 172 77 For example, the inner product of weight and height in \\(\\mathbf{X}\\) is \\[ \\texttt{weight}^\\mathsf{T} \\hspace{1mm} \\texttt{height} = (150 \\times 49) + (172 \\times 77) = 20594 \\] What does this value mean? To answer this question we need to discuss three other concepts that are directly derived from having an inner product: Length of a vector Angle between vectors Projection of vectors All these aspects play a very important role for multivariate methods. But not only that, we’ll see in a moment how many statistical summaries can be obtained through inner products. "],
["length.html", "6.2 Length", " 6.2 Length Another important usage of the inner product is that it allows us to define the length of a vector \\(\\mathbf{x}\\), denoted by \\(\\| \\mathbf{x} \\|\\), as the square root of the inner product of the vector with itself:: \\[ \\| \\mathbf{x} \\| = \\sqrt{\\mathbf{x^\\mathsf{T} x}} \\] which is typically known as the norm of a vector. We can calculate the length of the vector weight as: \\[ \\| \\texttt{weight} \\| = \\sqrt{(150 \\times 150) + (172 \\times 172)} = 228.2192 \\] Likewise, the length of the vector height is: \\[ \\| \\texttt{height} \\| = \\sqrt{(49 \\times 49) + (77 \\times 77)} = 91.2688 \\] Note that the inner product of a vector with itself is equal to its squared norm: \\[ \\mathbf{x^\\mathsf{T} x} = \\| \\mathbf{x} \\|^2 \\] "],
["angle.html", "6.3 Angle", " 6.3 Angle In addition to the length of a vector, the angle between two nonzero vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) can also be expressed using inner products. The angle \\(\\theta\\) is such that: \\[ cos(\\theta) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\sqrt{\\mathbf{x^\\mathsf{T} x}} \\hspace{1mm} \\sqrt{\\mathbf{y^\\mathsf{T} y}}} \\] or equivalently \\[ cos(\\theta) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\| \\mathbf{x} \\| \\hspace{1mm} \\| \\mathbf{y} \\|} \\] Rearranging some terms, we can reexpress the formula of the inner product as: \\[ \\mathbf{x^\\mathsf{T} y} = \\| \\mathbf{x} \\| \\hspace{1mm} \\| \\mathbf{y} \\| \\hspace{1mm} cos(\\theta) \\] The angle between weight and height in \\(\\mathbf{X}\\) is such that: \\[ cos(\\theta) = \\frac{20594}{228.2192 \\times 91.2688} = 0.9887 \\] "],
["orthogonality.html", "6.4 Orthogonality", " 6.4 Orthogonality Besides calculating lengths of vectors and angles between vectors, an inner product allows us to know whether two vectors are orthogonal. In a two dimensional space, orthogonality is equivalent to perpendicularity; so if two vectors are perpendicular to each other—the angle between them is a 90 degree angle—they are orthogonal. Two vectors vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal if their inner product is zero: \\[ \\mathbf{x^\\mathsf{T} y} = 0 \\iff \\mathbf{x} \\hspace{1mm} \\bot \\hspace{1mm} \\mathbf{y} \\] "],
["projection.html", "6.5 Projection", " 6.5 Projection The last aspect I want to touch related with the inner product is the so-called projections. The idea we need to consider is the orthogonal projection of a vector \\(\\mathbf{y}\\) onto another vector \\(\\mathbf{x}\\). The basic notion of projection requires two ingredients: two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). To obtain the projection of \\(\\mathbf{y}\\) onto \\(\\mathbf{x}\\), we need to express \\(\\mathbf{x}\\) in unit norm. The obtained projection \\(\\hat{\\mathbf{y}}\\) is expressed as \\(a \\mathbf{x}\\). This means that a projection implies multiplying \\(\\mathbf{x}\\) by some number \\(a\\), such that \\(\\hat{\\mathbf{y}} = a \\mathbf{x}\\) is a stretched version of \\(\\mathbf{x}\\). This is better appreciated in the following figure. Having two nonzero vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we can project \\(\\mathbf{y}\\) on \\(\\mathbf{x}\\): \\[ \\text{projection } \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\mathbf{x^\\mathsf{T} x}} \\right) \\] \\[ = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\| \\mathbf{x} \\|^2} \\right) \\] Likewise, we can project \\(\\mathbf{x}\\) on \\(\\mathbf{y}\\): \\[ \\text{proj}_{x} y \\mathbf{\\hat{x}} = \\mathbf{y} \\left( \\frac{\\mathbf{x^\\mathsf{T} y}}{\\mathbf{y^\\mathsf{T} y}} \\right) \\] \\[ = \\mathbf{y} \\left( \\frac{\\mathbf{x^\\mathsf{T} y}}{\\| \\mathbf{y} \\|^2} \\right) \\] "],
["mean.html", "Chapter 7 Mean", " Chapter 7 Mean The starting point in multivariate analysis consists in computing various summary measures—such as means, and variances—to get an idea of the common or central values, and the amount of variability of each variable. In this chapter you will learn how concepts like the mean of a variable can be expressed in terms of vector-matrix operations. "],
["mean-of-a-variable.html", "7.1 Mean of a variable", " 7.1 Mean of a variable To measure variation, we usually begin by calculating a “typical” value. The idea is to summarize the values of a variable with one or two representative values. You will find this notion under several terms like measures of center, location, central tendency, or centrality. The prototypical summary value of center is the mean, sometimes referred to as average. The mean of an \\(n-\\)element variable \\(X = (x_1, x_2, \\dots, x_n)\\), represented by \\(\\bar{x}\\), is obtained by adding all the \\(x_i\\) values and then dividing by their total number \\(n\\): \\[ \\bar{x} = \\frac{x_1 + x_2 + \\dots + x_n}{n} \\] Using summation notation we can express \\(\\bar{x}\\) in a very compact way as: \\[ \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] If we want to compute the mean value of weight \\[ \\overline{\\texttt{weight}} = \\frac{1}{5} (22 + 15 + 23 + 19 + 14) = 18.6 \\] If you associate a constant weight of \\(1/n\\) to each observation \\(x_i\\), you can look at the formula of the mean as a weighted sum: \\[ \\bar{x} = \\frac{1}{n} x_1 + \\frac{1}{n} x_2 + \\dots + \\frac{1}{n} x_n \\] This is a slightly different way of looking at the mean that will allow you to generalize the concept of an “average” as a weighted aggregation of information. For example, if we denote the weight of the \\(i\\)-th individual as \\(w_i\\), then the average can be expressed as: \\[ \\bar{x} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\sum_{i=1}^{n} w_i x_i \\] "],
["mean-as-a-balancing-point.html", "7.2 Mean as a Balancing Point", " 7.2 Mean as a Balancing Point What does the mean do? In what sense the mean is a “typical” value? How do you make sense of a mean value? To understand what the mean is doing, I find it helpful to visualize the values of a variable as points in a number-line, or with a dot-plot. Consider a variable having five numbers: \\(X = (1, 2, 6, 7, 9)\\), that can be visualized on a number line (see image below). Imagine that the number line is like a weighting scale or a teeter-totter (or see-saw). Figure 7.1: Values on a number-line The mean plays the role of the balancing point. In other words, \\(\\bar{x}\\) is the value that balances out the scale and keeps things on equilibrium. In this example, the value of 5 is the balancing point: Figure 7.2: Mean as a balancing point Saying that 5 is the balancing point has a specific meaning. It means that the signed distances of all the values with respect to the mean cancel out. If you calculate the deviations from the mean: \\((x_i - \\bar{x})\\), and add them all up, the sum is zero: \\[ (1 - 5) + (2 - 5) + (6 - 5) + (7 - 5) + (9 - 5) = 0 \\] This is a very special property of the mean, indicating that this value is optimal in the sense that it is the only number for which the sum of deviations with itself is zero. In a more algebraic way, say you are looking for a number \\(a\\) that makes the sum of deviations zero: \\[ \\sum_{i=1}^{n} (x_i - a) = 0 \\] What would \\(a\\) be? Then mean \\(\\bar{x}\\)! No other value would be able to cancel out the sum of deviations around itself. Keep in mind that the mean is just a summary. In fact, it is just one possible kind of “typical” value (other common measures of center are the median or the mode). As any summary statistic, it involves compressing the information of a variable into a single representative number. But remember: this number does not tell you the whole story about the variability in a variable. "],
["mean-with-vector-notation.html", "7.3 Mean with Vector Notation", " 7.3 Mean with Vector Notation It is very useful to be able to compute a mean using vector-matrix notation. First, notice that the formula of the mean consists of computing a (weighted) sum. Second, recall that a sum of numbers can be expressed with an inner product by using the unit vector (or summation operator). If we denote \\(\\mathbf{1}_{n}\\) a vector of ones of size \\(n\\), then the mean value of a vector \\(\\mathbf{x}\\) can be obtained with an inner product: \\[ \\bar{x} = \\left(\\frac{1}{n}\\right) \\mathbf{1}_{n}^\\mathsf{T} \\mathbf{x} \\] As you can tell, \\(\\bar{x}\\) is calculated by multiplying the scalar \\(1/n\\) with the inner product \\(\\mathbf{1}_{n}^\\mathsf{T} \\mathbf{x}\\), which is equivalent to: \\[ \\bar{x} = \\left(\\frac{1}{n}\\right) &lt;\\mathbf{1}_{n}, \\mathbf{x}&gt; = \\left(\\frac{1}{n}\\right) &lt;\\mathbf{x}, \\mathbf{1}_{n}&gt; \\] This way of using an inner product can also be genelarized with the notation \\(&lt;\\mathbf{a}, \\mathbf{b}&gt;_{M}\\) which is simply an inner product in a vector space endowed with a metric matrix \\(\\mathbf{M}\\) \\[ &lt;\\mathbf{a}, \\mathbf{b}&gt;_{M} = \\mathbf{a}^\\mathsf{T} \\mathbf{M b} \\] If we use a metric matrix \\(\\mathbf{D} = diag(1/n)\\) then we have that the mean is given by: \\[ \\bar{x} = &lt;\\mathbf{x}, \\mathbf{1}&gt;_{D} = \\mathbf{x}^\\mathsf{T} \\mathbf{D 1} \\] "],
["mean-as-a-projection.html", "7.4 Mean as a Projection", " 7.4 Mean as a Projection We can gain insight to what a mean is doing if we regard a variable from the perspective of the individuals’ space. This interpretation is less common than the balancing point, but it is equally revealing and informative. Consider the \\(n\\)-dimensional space \\(\\mathbb{R}^n\\) illustrated in the figure below. In this space, each dimension is associated to an individual. The variable \\(X\\) is represented by the blue vector \\(\\mathbf{x}\\). In turn, the orange vector represents the \\(n\\)-element unit-vector \\(\\mathbf{1}\\). From this point of view, the mean acquires a very interesting meaning: it turns out that the mean is the measure of the projection of \\(\\mathbf{x}\\) onto the axis spanned by the unit vector \\(\\mathbf{1}\\). Figure 7.3: Mean as the measure of a projection So what is the meaning of this orthogonal projection? It tells you that \\(\\bar{x} \\mathbf{1}\\) is the multiple of the unit-vector that is the closest to \\(\\mathbf{x}\\) in the least squares sense: Figure 7.4: A variable and its mean vector The size of the residual vector \\(\\mathbf{e} = \\mathbf{x} - \\bar{x} \\mathbf{1}\\) is the smallest among all other multiples of \\(\\mathbf{1}\\) that you can use to approximate \\(\\mathbf{x}\\). "],
["centroid.html", "7.5 Centroid", " 7.5 Centroid So far we’ve considered the mean of a single variable \\(X\\), which gives us one measure of center or measure of a typical individual. But what about the mean in a multivariate sense? If we have more than one variable, say \\(p\\) variables \\(X_1, \\dots, X_p\\), is there an extension of the notion of typical individual? If we have several variables or vectors of the same size, like in a \\(n \\times p\\) matrix \\(\\mathbf{X}\\), we can get the mean vector also known as centroid. The row vector of means of \\(\\mathbf{X}\\) is denoted by: \\[ \\mathbf{\\bar{x}} = \\frac{1}{n} \\mathbf{1}_{n}^\\mathsf{T} \\mathbf{X} \\] and it will be a vector containing the means of of all variables: \\[ \\mathbf{\\bar{x}} = (\\bar{x}_1, \\bar{x}_2, \\dots, \\bar{x}_p) \\] "],
["variance.html", "Chapter 8 Variance", " Chapter 8 Variance A measure of center such as the mean is not enoguh to summarize the information of a variable. We also need a measure of the amount of variability. Synonym terms are variation, spread, scatter, and dispersion. There are several ways to measure spread: overall range: \\(max(X) - min(X)\\) interquartile range \\(Q_3(X) - Q_1(X)\\) the length between two quantiles variance (and its square root the standard deviation) Because of its relevance and importance for statistical learning methods, we will focus on the variance. "],
["about-the-variance.html", "8.1 About the variance", " 8.1 About the variance Simply put, the variance is a measure of spread around the mean. The main idea behind the calculation of the variance is to quantify the typical concentration of values around the mean. The way this is done is by averaging the squared deviations from the mean. \\[ var(X) = \\frac{(x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Let’s disect the terms and operations involved in the formula of the variance. the main terms are the deviations from the mean \\((x_i - \\bar{x})\\), that is, the difference between each observation \\(x_i\\) and the mean \\(\\bar{x}\\). conceptually speaking, we want to know what is the average size of the deviations around the mean. simply averaging the deviations won’t work because their sum is zero (i.e. the sum of deviations around the mean will cancel out because the mean is the balancing point). this is why we square each deviation: \\((x_i - \\bar{x})^2\\), which literally means getting the squared distance from \\(x_i\\) to \\(\\bar{x}\\). having squared all the deviations, then we average them to get the variance. Because the variance has squared units, we need to take the square root to “recover” the original units in which \\(X\\) is expressed. This gives us the standard deviation \\[ sd(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\] In this sense, you can say that the standard deviation is roughly the average distance that the data points vary from the mean. 8.1.1 Sample Variance In practice, you will often find two versions of the formula for the variance: one in which the sum of squared deviations is divided by \\(n\\), and another one in which the division is done by \\(n-1\\). Each version is associated to the statistical inference view of variance in terms of whether the data comes from the population or from a sample of the population. The population variance is obtained dividing by \\(n\\): \\[ \\textsf{population variance:} \\quad \\frac{1}{(n)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] The sample variance is obtained dividing by \\(n - 1\\) instead of dividing by \\(n\\). The reason for doing this is to get an unbiased estimor of the population variance: \\[ \\textsf{sample variance:} \\quad \\frac{1}{(n-1)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] It is important to note that most statistical software compute the variance with the unbiased version. This is also the case in R with the function var(). For instance, consider the toy data set in the following matrix X # data matrix X &lt;- matrix(c(150, 172, 180, 49, 77, 80), nrow = 3, ncol = 2) rownames(X) &lt;- c(&quot;Leia&quot;, &quot;Luke&quot;, &quot;Han&quot;) colnames(X) &lt;- c(&quot;weight&quot;, &quot;height&quot;) X weight height Leia 150 49 Luke 172 77 Han 180 80 The unbiased variance (i.e. sample variance) of weight is: # variance of shots var(X[,1]) [1] 241 Compare it to the biased variance: # biased variance of shots (nrow(X) - 1) / (nrow(X)) * var(X[,1]) [1] 161 As you can tell from the two types of variances, there seems to be an important difference. This is because the number of observations \\(n = 5\\) in this case is small. However, as the sample size increases, the difference between \\(n-1\\) and \\(n\\) will be negligible. If you implement your own functions and are planning to compare them against other software, then it is crucial to known what other programmers are using for computing the variance. Otherwise, your results might be a bit different from the ones with other people’s code. In this book, to keep notation as simpler as possible, we will use the factor \\(\\frac{1}{n}\\) for the rest of the formulas. However, keep in mind that most variance-based computations in R use \\(\\frac{1}{n-1}\\). "],
["variance-with-vector-notation.html", "8.2 Variance with Vector Notation", " 8.2 Variance with Vector Notation In a similar way to expressing the mean with vector notation, you can also formulate the variance in terms of vector-matrix notation. First, notice that the formula of the variance consists of the addition of squared terms. Second, recall that a sum of numbers can be expressed with an inner product by using the unit vector (or summation operator). If we denote a vector of ones of size \\(n\\) as \\(\\mathbf{1}_{n}\\), then the variance of a vector \\(\\mathbf{x}\\) can be obtained with the following inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} (\\mathbf{x} - \\mathbf{\\bar{x}})^\\mathsf{T} (\\mathbf{x} - \\mathbf{\\bar{x}}) \\] where \\(\\mathbf{\\bar{x}}\\) is an \\(n\\)-element vector of mean values \\(\\bar{x}\\). Assuming that \\(\\mathbf{x}\\) is already mean-centered, then the variance is proportional to the squared norm of \\(\\mathbf{x}\\) \\[ var(\\mathbf{x}) = \\frac{1}{n} \\hspace{1mm} \\mathbf{x}^\\mathsf{T} \\mathbf{x} = \\frac{1}{n} \\| \\mathbf{x} \\|^2 \\] This means that we can formulate the variance with the general notion of an inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} &lt;\\mathbf{x}, \\mathbf{x}&gt; \\] "],
["standard-deviation-as-a-norm.html", "8.3 Standard Deviation as a Norm", " 8.3 Standard Deviation as a Norm If we use a metric matrix \\(\\mathbf{D} = diag(1/n)\\) then we have that the variance is given by a special type of inner product: \\[ var(\\mathbf{x}) = &lt;\\mathbf{x}, \\mathbf{x}&gt;_{D} = \\mathbf{x}^\\mathsf{T} \\mathbf{D x} \\] From this point of view, we can say that the variance of \\(\\mathbf{x}\\) is equivalent to its squared norm when the vector space is endowed with a metric \\(\\mathbf{D}\\). Consequently, the standard deviation is simply the length of \\(\\mathbf{x}\\) in this particular geometric space. \\[ sd(\\mathbf{x}) = \\| \\mathbf{x} \\|_{D} \\] When looking at the standard deviation from this perspective, you can actually say that the amount of spread of a vector \\(\\mathbf{x}\\) is actually its length (under the metric \\(\\mathbf{D}\\)). "],
["covar.html", "Chapter 9 Covariance and Correlation", " Chapter 9 Covariance and Correlation One of the most common tasks in statistics and data analysis that we have to deal with is answering the ever-present question: Given two variables \\(X\\) and \\(Y\\), what is the relationship between them? The answer to this question largely depends on two major factors. One factor is the way in which the notion of “relationship” is defined in mathematical terms. The other factor has to do with the nature of the variables (e.g. continuous, discrete, ordinal, nominal, binary, etc). "],
["two-variables.html", "9.1 Two Variables", " 9.1 Two Variables Let us start by considering the most simple scenario when we have two variables \\(X\\) and \\(Y\\), represented by vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), respectively. Different types of relations between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are possible, as shown in the figure below containing several scatter-plots. There are linear relations of both type positive and negative. There are also two types of nonlinear relations, one monotone and the other one non-montone. In addition, the last two plots show absence of relations. Figure 9.1: Different relations between two variables Positive linear relation: \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) vary simultaneously in the same direction; an increase in \\(\\mathbf{x}\\) is accompanied by an increase in \\(\\mathbf{y}\\) in constant proportion. Negative linear relation: \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) vary in opposite directions; an increase in \\(\\mathbf{x}\\) is accompanied by a decrease in \\(\\mathbf{y}\\). Nonlinear monotone relation: \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) vary in the same direction but the slope is not constant; changes in \\(\\mathbf{y}\\) are different dependening on the values of \\(\\mathbf{x}\\). Nonlinear non-monotone relation: although there is a functional relation between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the relation is not monotone; \\(\\mathbf{y}\\) increases and decrases accordingly to \\(\\mathbf{x}\\). No relation: regardless of the values of \\(\\mathbf{x}\\), two things may happen. One is that knowing \\(\\mathbf{x}\\), nothing can be said about \\(\\mathbf{y}\\); the other is that \\(\\mathbf{y}\\) remains constant. For convenience sake most multivariate techniques focus on linear relations. Even though two variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) may show a nonlinear relation, it is often possible to apply some transformation to one of the them in order to have a more linear association. The general approach to determine if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are related is to assess whether there is simultaneous variation between them. The idea is to define a measure for quantifying the strength of the relation. When \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are in a quantitative scale, the most common way to quantify and assess the relationship between them is with the covariance and correlation. "],
["covariance.html", "9.2 Covariance", " 9.2 Covariance The covariance generalizes the concept of variance for two variables. Recall that the formula for the covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y}) \\] where \\(\\bar{x}\\) is the mean value of \\(\\mathbf{x}\\) obtained as: \\[ \\bar{x} = \\frac{1}{n} (x_1 + x_2 + \\dots + x_n) = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] and \\(\\bar{y}\\) is the mean value of \\(\\mathbf{y}\\): \\[ \\bar{y} = \\frac{1}{n} (y_1 + y_2 + \\dots + y_n) = \\frac{1}{n} \\sum_{i = 1}^{n} y_i \\] Basically, the covariance is a statistical summary that is used to assess the linear association between pairs of variables. Assuming that the variables are mean-centered, we can get a more compact expression of the covariance in vector notation: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} (\\mathbf{x^\\mathsf{T} y}) \\] Properties of covariance: the covariance is a symmetric index: \\(cov(X,Y) = cov(Y,X)\\) the covariance can take any real value (negative, null, positive) the covariance is linked to variances under the name of the Cauchy-Schwarz inequality: \\[cov(X,Y)^2 \\leq var(X) var(Y) \\] "],
["correlation.html", "9.3 Correlation", " 9.3 Correlation Although the covariance indicates the direction—positive or negative—of a possible linear relation, it does not tell us how big or small the relation might be. To have a more interpretable index, we must transform the convariance into a unit-free measure. To do this we must consider the standard deviations of the variables so we can normalize the convariance. The result of this normalization is the coefficient of linear correlation defined as: \\[ cor(X, Y) = \\frac{cov(X, Y)}{\\sqrt{var(X)} \\sqrt{var(Y)}} \\] Representing \\(X\\) and \\(Y\\) as vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we can express the correlation as: \\[ cor(\\mathbf{x}, \\mathbf{y}) = \\frac{cov(\\mathbf{x}, \\mathbf{y})}{\\sqrt{var(\\mathbf{x})} \\sqrt{var(\\mathbf{y})}} \\] Assuming that \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are mean-centered, we can express the correlation as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\] As it turns out, the norm of a mean-centered variable \\(\\mathbf{x}\\) is proportional to the square root of its variance (or standard deviation): \\[ \\| \\mathbf{x} \\| = \\sqrt{\\mathbf{x^\\mathsf{T} x}} = \\sqrt{n} \\sqrt{var(\\mathbf{x})} \\] Consequently, we can also express the correlation with inner products as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\sqrt{(\\mathbf{x^\\mathsf{T} x})} \\sqrt{(\\mathbf{y^\\mathsf{T} y})}} \\] or equivalently: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\| \\mathbf{x} \\| \\hspace{1mm} \\| \\mathbf{y} \\|} \\] In the case that both \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are standardized (mean zero and unit variance), that is: \\[ \\mathbf{x} = \\begin{bmatrix} \\frac{x_1 - \\bar{x}}{\\sigma_{x}} \\\\ \\frac{x_2 - \\bar{x}}{\\sigma_{x}} \\\\ \\vdots \\\\ \\frac{x_n - \\bar{x}}{\\sigma_{x}} \\end{bmatrix}, \\hspace{5mm} \\mathbf{y} = \\begin{bmatrix} \\frac{y_1 - \\bar{y}}{\\sigma_{y}} \\\\ \\frac{y_2 - \\bar{y}}{\\sigma_{y}} \\\\ \\vdots \\\\ \\frac{y_n - \\bar{y}}{\\sigma_{y}} \\end{bmatrix} \\] the correlation is simply the inner product: \\[ cor(\\mathbf{x, y}) = \\mathbf{x^\\mathsf{T} y} \\hspace{5mm} \\mathrm{(standardized} \\hspace{1mm} \\mathrm{variables)} \\] Let’s look at two variables (i.e. vectors) from a geometric perspective. Figure 9.2: Two vectors in a 2-dimensional space The inner product ot two mean-centered vectors \\(\\mathbf{x&#39;y}\\) is obtained with the following equation: \\[ \\mathbf{x^\\mathsf{T} y} = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| cos(\\theta_{x,y}) \\] where \\(cos(\\theta_{x,y})\\) is the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Rearranging the terms in the previous equation we get that: \\[ cos(\\theta_{x,y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} = cor(\\mathbf{x, y}) \\] which means that the correlation between mean-centered vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) turns out to be the cosine of the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). "],
["relation-as-linear-approximation.html", "9.4 Relation as Linear Approximation", " 9.4 Relation as Linear Approximation A linked but different way to examine relations between the variables is when we ask: Can we approximate one of the variables in terms of the other? This is an asymmetric type of association since we seek to say something about the variability of one variable, say \\(\\mathbf{y}\\), in terms of the variability of \\(\\mathbf{x}\\). We can think of several ways to approximate \\(\\mathbf{y}\\) in terms of \\(\\mathbf{x}\\). The approximation of \\(\\mathbf{y}\\), denoted by \\(\\hat{\\mathbf{y}}\\), means finding a coefficient \\(b\\) such that: \\[ \\hat{\\mathbf{y}} = b \\mathbf{x} \\] The common approach to get \\(\\hat{\\mathbf{y}}\\) in some optimal way is by minimizing the square difference between \\(\\mathbf{y}\\) and \\(\\hat{\\mathbf{y}}\\). Figure 9.3: Linear approximation The answer to this question comes in the form of a projection. More precisely, we orthogonally project \\(\\mathbf{y}\\) onto \\(\\mathbf{x}\\): \\[ \\hat{\\mathbf{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\mathbf{x^\\mathsf{T} x}} \\right) \\] or equivalently: \\[ \\hat{\\mathbf{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\| \\mathbf{x} \\|^2} \\right) \\] Note that the term in parenthesis is just a scalar, so we can actually express \\(\\hat{\\mathbf{y}}\\) as \\(b \\mathbf{x}\\). This means that a projection implies multiplying \\(\\mathbf{x}\\) by some number \\(b\\), such that \\(\\hat{\\mathbf{y}} = b \\mathbf{x}\\) is a stretched or shrinked version of \\(\\mathbf{x}\\). This is nothing else than the least squares regression of \\(\\mathbf{y}\\) on \\(\\mathbf{x}\\). This is better appreciated in the following figure. Figure 9.4: Orthogonal projection Note that the correlation between \\(\\mathbf{y}\\) and \\(\\hat{\\mathbf{y}}\\) is: \\[ cor(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\mathbf{y^\\mathsf{T} x}}{\\| \\mathbf{y} \\|} \\] or alternatively: \\[ cor^{2}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\mathbf{y^\\mathsf{T} x}}{\\mathbf{y^\\mathsf{T}}y} \\] "],
["crossprods.html", "Chapter 10 Matrix Cross-Products", " Chapter 10 Matrix Cross-Products Until now, we have been discussing statistical measures pertaining to one and two variables: things like mean, variance, covariance, and correlation. All of these summaries can be expressed with vector notation, and using common vector operations such as inner products, norms, angles, and projections. However, we can also use the entire data matrix to obtain other types of statistical measures. In this chapter, we’ll describe statistical objects that are based on cross-products of a data matrix. "],
["common-data-matrices.html", "10.1 Common Data Matrices", " 10.1 Common Data Matrices To begin our discussion we need to consider a data matrix \\(\\mathbf{X}\\) formed by \\(p\\) variables measured on \\(n\\) individuals. For convenience, we will also assume that the variables are quantitative. In other words, we assume that the data matrix contains columns with (roughly) continuous values. If the raw data matrix has categorical variables with string values or coded in a non-numerical way, you would have to transform and codify such variables to end up with a numeric data matrix. One general way to depict \\(\\mathbf{X}\\) is as: \\[ \\underset{n \\times p}{\\mathbf{X}} = \\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\\\ \\end{array}\\right] \\] Centered Matrix If the variables are mean-centered, then \\(\\mathbf{X}\\) become \\(\\mathbf{X_C}\\) with elements: \\[ \\underset{n \\times p}{\\mathbf{X_C}} = \\left[\\begin{array}{cccc} x_{11} - \\bar{x}_1 &amp; x_{12} - \\bar{x}_2 &amp; \\cdots &amp; x_{1p} - \\bar{x}_p \\\\ x_{21} - \\bar{x}_1 &amp; x_{22} - \\bar{x}_2 &amp; \\cdots &amp; x_{2p} - \\bar{x}_p \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} - \\bar{x}_1 &amp; x_{n2} - \\bar{x}_2 &amp; \\cdots &amp; x_{np} - \\bar{x}_p \\\\ \\end{array}\\right] \\] where \\(\\bar{x}_j\\) is the mean of the \\(j\\)-th variable (\\(j = 1, \\dots, p\\)). Using matrix notation, the centering operation is expressed as: \\[ \\mathbf{X_C} = (\\mathbf{I} - \\frac{1}{n} \\mathbf{11^\\mathsf{T}}) \\mathbf{X} \\] where: \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix \\(\\mathbf{1}\\) is an \\(n \\times 1\\) vector of ones \\(\\mathbf{I} - \\frac{1}{n} \\mathbf{11^\\mathsf{T}}\\) is sometimes called the centering operator Normalized Matrix Another special matrix is the scaled or normalized matrix \\(\\mathbf{X_N}\\): \\[ \\underset{n \\times p}{\\mathbf{X_N}} = \\left[\\begin{array}{cccc} a_1 x_{11} &amp; a_2 x_{12} &amp; \\cdots &amp; a_p x_{1p} \\\\ a_1 x_{21} &amp; a_2 x_{22} &amp; \\cdots &amp; a_p x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_1 x_{n1} &amp; a_2 x_{n2} &amp; \\cdots &amp; a_p x_{np} \\\\ \\end{array}\\right] \\] where \\(a_j\\) is a scaling factor for the \\(j\\)-th column Probably the most common scaling option is to divide by the standard deviation: \\[ a_j = \\frac{1}{sd_j} = \\frac{1}{\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2}} \\] The scaling factors \\(a_j\\) can be put in a diagonal matrix \\(\\mathbf{D_a}\\) \\[ \\underset{p \\times p}{\\mathbf{D_a}} = \\left[\\begin{array}{cccc} a_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_p \\\\ \\end{array}\\right] \\] then the scaled or normalized data matrix is given by: \\[ \\mathbf{X_N} = \\mathbf{X D_a} \\] Standardized Matrix The standardized matrix \\(\\mathbf{X_S}\\) is the mean-centered and scaled (by the standard deviation) matrix: \\[ \\underset{n \\times p}{\\mathbf{X_S}} = \\left[\\begin{array}{cccc} \\frac{x_{11} - \\bar{x}_1}{sd_1} &amp; \\frac{x_{12} - \\bar{x}_2}{sd_2} &amp; \\cdots &amp; \\frac{x_{1p} - \\bar{x}_p}{sd_p} \\\\ \\frac{x_{21} - \\bar{x}_1}{sd_1} &amp; \\frac{x_{22} - \\bar{x}_2}{sd_2} &amp; \\cdots &amp; \\frac{x_{2p} - \\bar{x}_p}{sd_p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{x_{n1} - \\bar{x}_1}{sd_1} &amp; \\frac{x_{n2} - \\bar{x}_2}{sd_2} &amp; \\cdots &amp; \\frac{x_{np} - \\bar{x}_p}{sd_p} \\\\ \\end{array}\\right] \\] \\(\\bar{x}_j\\) is the mean of the \\(j\\)-th variable \\(sd_j\\) is the standard deviation of the \\(j\\)-th variable When the scaling factors \\(a_j\\) are the standard deviations \\(sd_j\\), the scaling matrix \\(\\mathbf{D}_{\\frac{1}{sd}}\\) is: \\[ \\underset{p \\times p}{\\mathbf{D}_{\\frac{1}{sd}}} = \\left[\\begin{array}{cccc} \\frac{1}{sd_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{sd_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{sd_p} \\\\ \\end{array}\\right] \\] then the standardized data matrix \\(\\mathbf{X_S}\\) \\[ \\mathbf{X_S} = \\mathbf{X_C} \\mathbf{D}_{\\frac{1}{sd}} = (\\mathbf{I} - \\frac{1}{n} \\mathbf{11^\\mathsf{T}}) \\mathbf{X} \\mathbf{D}_{\\frac{1}{sd}} \\] "],
["cross-products.html", "10.2 Cross-Products", " 10.2 Cross-Products There are two fundamental matrix products that play a crucial role when the data is in an \\(n \\times p\\) matrix \\(X\\) with objects in rows, and variables in columns (assume \\(n &gt; p\\)): \\[ \\mathbf{X^\\mathsf{T} X} \\quad \\&amp; \\quad \\mathbf{X X^\\mathsf{T}} \\] \\(\\mathbf{X^\\mathsf{T} X}\\) is also known as the minor product moment because it is of size \\(p \\times p\\) (assuming \\(n &gt; p\\)). sum-of-squares and cross-products (SSCP) of columns made of inner products of the columns of \\(\\mathbf{X}\\) association matrix for the variables \\(\\mathbf{X X^\\mathsf{T}}\\) is also known as the major product moment because is of size \\(n \\times n\\) (assuming \\(n &gt; p\\)). sum-of-squares and cross-products of rows made of inner products of the rows of \\(\\mathbf{X}\\) association matrix for the objects Covariance Matrix If \\(\\mathbf{X}\\) is mean-centered, i.e. \\(\\mathbf{X} = \\mathbf{X_C}\\), then \\[ \\frac{1}{n} \\mathbf{X^\\mathsf{T} X} \\qquad \\text{and} \\qquad \\frac{1}{n-1} \\mathbf{X^\\mathsf{T} X} \\] are the covariance matrices (population and sample flavors). Correlation Matrix If \\(\\mathbf{X}\\) is standardized, i.e. \\(\\mathbf{X} = \\mathbf{X_S}\\), then \\[ \\frac{1}{n} \\mathbf{X^\\mathsf{T} X} \\qquad \\text{and} \\qquad \\frac{1}{n-1} \\mathbf{X^\\mathsf{T} X} \\] are the correlation matrices (population and sample flavors). "],
["decomps.html", "Chapter 11 Matrix Decompositions", " Chapter 11 Matrix Decompositions This part of the book is dedicated to matrix decompositions. This is a central topic in matrix algebra, and it is also a fundamental tool for many statistical learning methods. Decompositions, even though they may seem obscure and scary the first time we encounter them, are our friends. Thanks to them we can analyze a great variety of data sets and do many great things that would otherwise be impossible. "],
["decompositions.html", "11.1 Decompositions", " 11.1 Decompositions Simply put, decompositions are like factorizations of numbers. For example, consider the number 6; we can decompose 6 as the product of 3 and 2: \\[ 6 = 3 \\times 2 \\] that is, we factorize 6 in terms of “simpler” numbers. If the idea of factorizing numbers seems too simplistic to you, then consider the factorization of polynomials. Here’s one toy example with the following polynomial: \\[ p(x) = x^2 + 5x + 6 \\] The above quadratic polynomial can be decomposed as the product of two linear—and therefore more basic—polynomials: \\[ x^2 + 5x + 6 = (x + 3) (x + 2) \\] This factorization allows us to dismantle the quadratic polynomial into a product of polynomials of smaller degree. In particular, we observe that the two resulting terms contain the roots of the original polynomial: 3 and 2. The same idea applies to matrix decompositions: they allow us to break down a matrix and decompose it in its main parts. They make it easier to study the properties of matrices, and they tend to make matrix computations easier. 11.1.1 General Idea More formally, a matrix decomposition is a way of expressing a matrix \\(\\mathbf{M}\\) as the product of a set of new—typically two or three—matrices, usually simpler in some sense, that gives us an idea of the inherent structures or relationships in \\(\\mathbf{M}\\). By “simpler” we mean a series of ideas such as more compact matrices, or with less dimensions, or with less number of rows, or less number of columns; perhaps triangular matrices, or even matrices almost full of zeros, except in their diagonal. Roughly speaking, the decomposition of an \\(n \\times p\\) matrix \\(\\mathbf{M}\\) can be described by a generic equation of the form: \\[ \\mathbf{M} = \\mathbf{A B C} \\] where the dimensions of the matrices are as follows: \\(\\mathbf{M}\\) is \\(n \\times p\\) (we assume for simplicity that \\(n &gt; p\\)) \\(\\mathbf{A}\\) is \\(n \\times k\\) (usually \\(k &lt; p\\)) \\(\\mathbf{B}\\) is \\(k \\times k\\) (usually diagonal) \\(\\mathbf{C}\\) is \\(k \\times p\\) The following figure illustrates a matrix decomposition from the point of view of matrix shapes: Figure 11.1: Matrix Decomposition Diagram As you can tell, the matrix \\(\\mathbf{A}\\) has the same number of rows as \\(\\mathbf{M}\\). This is not a coincidence. In fact, each of the rows in \\(\\mathbf{A}\\) provides information about the object described by the corresponding row of \\(\\mathbf{M}\\). In this sense, the \\(i\\)-th row of \\(\\mathbf{A}\\) involves \\(k\\) bits of information that shed light on the data of the \\(i\\)-th object. The matrix \\(\\mathbf{C}\\) has the same number of columns as \\(\\mathbf{M}\\). In the same way that \\(\\mathbf{A}\\) has to do with the rows of \\(\\mathbf{M}\\), the matrix \\(\\mathbf{C}\\) has to do with the columns of \\(\\mathbf{M}\\). Each column of \\(\\mathbf{C}\\) gives a different perspective of the variable described by the corresponding column of \\(\\mathbf{M}\\), in terms of \\(k\\) bits of information. Observe that the common element in matrices \\(\\mathbf{A}\\) and \\(\\mathbf{C}\\) is previsely the constant \\(k\\). The role of \\(k\\) is to coerce a more comptact representation for the data as compared to its original form. While \\(k = p\\) might give a reasonable factorization, often \\(k\\) is chosen to be smaller than \\(p\\). We do this under the assumption that using less dimensions will still capture the intrinsic patterns in the data that might not be evident by direct inspection of \\(\\mathbf{M}\\). What about the middle matrix \\(\\mathbf{B}\\)? This matrix cements the connections between the pieces of information i\\(\\mathbf{A}\\) and \\(\\mathbf{C}\\). Unless stated otherwise, we will assume that \\(\\mathbf{B}\\) is diagonal. Now, keep in mind that some decompositions do not explicitly include this middle matrix. When that’s the case, we can always suppose the existance of a \\(k \\times k\\) identity matrix \\(\\mathbf{I}\\) such that we still get the general decomposition given by three matrices: \\[ \\mathbf{M} = \\mathbf{AC} = \\mathbf{A I C} \\] 11.1.2 Some Comments The equation that describes a decomposition: \\[ \\mathbf{M} = \\mathbf{A B C} \\] does not explain how to compute one, or how such decomposition can reveal the structures implicit in a data matrix. Nowadays, the computation of a matrix decomposition is straightforward; software to compute each one is readily available, and understanding how the algorithms work is not necessary to be able to interpret the results. Seeing how a matrix decomposition reveals structure in a dataset is more complicated. Each decomposition reveals a different kind of implicit structure and, for each decomposition, there are various ways to interpret the results. There are many different types of matrix decompositions, and each of them reveal different kinds of underlying structure. We are going to focus our discussion on two of them: the Singular Value Decomposition (SVD), and the Eigen Value Decomposition (EVD). Why these two decompositions? Because we concentrate on the two types of matrices more important in statistics: general rectangular matrices used to represent data tables, and positive semi-definite matrices used to represent covariance matrices, correlation matrices, and any matrix that results from a crossproduct. "],
["rank.html", "11.2 Rank", " 11.2 Rank There is one general concept which plays a fundamental role in many decompositions: the rank. If \\(\\mathbf{X}\\) is a \\(n \\times p\\) matrix, then \\(\\mathbf{X}\\) is of full column rank if \\(\\mathbf{Xb} = \\mathbf{0}\\) only if \\(\\mathbf{b} = \\mathbf{0}\\). We also say that the columns of \\(\\mathbf{X}\\) are linearly independent. Full row rank is defined in a similar way. If \\(\\mathbf{X}\\) can be written as the product \\(\\mathbf{AB}\\), with \\(\\mathbf{A}\\) an \\(n \\times k\\) matrix of full column rank and \\(\\mathbf{B}\\) an \\(k \\times p\\) matrix of full row rank, then \\(\\mathbf{X}\\) is said to have rank \\(k\\). The decomposition \\(\\mathbf{X} = \\mathbf{AB}\\) is a full rank decomposition. The rank of a matrix is the minimum number of row or column vectors needed to generate the rows or columns of the matrix exactly through linear combinations. Geometrically, this algebraic concept is equivalent to the dimensionality of the matrix. In practice, however, no large matrix is of low rank, but we can approximate it “optimally” by a matrix of low rank and then view this approximate matrix in a low-dimensional space. Suppose that \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix with rank \\(r\\). Then the idea of matrix approximation is to find another \\(n \\times p\\) matrix \\(\\mathbf{\\hat{X}}\\) of lower rank \\(k &lt; r\\) that resembles \\(\\mathbf{X}\\) “as closely as” possible. Closeness can be measured in any reasonable way, but least-squares approximation makes the solution of the problem particularly simple. Hence we want to find a matrix \\(\\mathbf{\\hat{X}}\\) that minimizes the following objective function over all possible rank \\(k\\) matrices: \\[ trace [ (\\mathbf{X} - \\mathbf{\\hat{X}} ) (\\mathbf{X} - \\mathbf{\\hat{X}} )^\\mathsf{T} ] \\] "],
["evd.html", "Chapter 12 Eigenvalue Decomposition", " Chapter 12 Eigenvalue Decomposition Eigenvalue decompositions play a fundamental role in statistics. This is particularly true in the case of multivariate analysis where eigenvalues of cross-product matrices (e.g. covariance matrices, correlations matrices) allow you to obtain simplified structures of data matrices. "],
["eigenvectors-and-eigenvalues.html", "12.1 Eigenvectors and Eigenvalues", " 12.1 Eigenvectors and Eigenvalues Let’s begin with the classic definition of eigenvectors and eigenvalues. Consider a square matrix \\(\\mathbf{A}\\) of order \\(p \\times p\\). Any vector \\(\\mathbf{v}\\) such that: \\[ \\mathbf{Av} = \\lambda \\mathbf{v} \\] with \\(\\lambda \\neq 0\\) some scalar, is called an eigenvector of \\(\\mathbf{A}\\), and \\(\\lambda\\) an eigenvalue. For example, consider the following matrix \\(\\mathbf{A}\\) and vector \\(\\mathbf{v_1}\\): \\[ \\mathbf{A} = \\ \\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; 1 \\\\ \\end{bmatrix}, \\ \\qquad \\mathbf{v_1} = \\ \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} \\] The vector resulting from multiplying \\(\\mathbf{Av_1}\\) is: \\[ \\mathbf{A v_1} = \\ \\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; 1 \\\\ \\end{bmatrix} \\ \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} \\ = \\begin{bmatrix} 5 \\\\ 5 \\\\ \\end{bmatrix} = 5 \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} \\] As you can tell, \\[ \\mathbf{Av_1} = 5 \\mathbf{v_1} \\] Therefore, \\(\\mathbf{v_1}\\) and \\(\\lambda_1 = 5\\) are an eigenvector and eigenvalue, respectively, of \\(\\mathbf{A}\\). More formally, if we think of a square matrix \\(\\mathbf{A}\\) as a transformation, an eigenvector \\(\\mathbf{v}\\) of \\(\\mathbf{A}\\) is a special kind of vector: it is a vector which under the transformation \\(\\mathbf{A}\\) maps into itself or a multiple of itself. Another way to put it is by saying that eigenvectors are invariant vectors under a given transformation. 12.1.1 Where do eigenvectors come from? The definition of an eigenvector and an eigenvalue tells us what they are, but it does not tell us where they come from. To understand how eigen-elements come into existance, we need to take a closer look at the matrix equation: \\[ \\mathbf{Av} = \\lambda \\mathbf{v} \\] We can rearrange the terms as follows: \\[ \\mathbf{Av} - \\lambda \\mathbf{v} = \\mathbf{0} \\] or equivalently: \\[ \\mathbf{Av} - \\lambda \\mathbf{Iv} = \\mathbf{0} \\] Notice that \\(\\mathbf{0}\\) is not the scalar 0, but a \\(p\\)-element vector of zeros. Likewise, the matrix \\(\\mathbf{I}\\) is the \\(p \\times p\\) identity matrix. Next, we can factor out \\(\\mathbf{v}\\) to get: \\[ \\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{v} = \\mathbf{0} \\] This matrix equation involves a system of \\(p\\) homogeneous equations with the elements of \\(\\mathbf{v}\\) as \\(p\\) unknowns. The equations can be solved only if the matrix of coefficients has rank smaller than \\(p\\); so the determinant of the \\(p \\times p\\) matrix \\(( \\mathbf{A} - \\lambda \\mathbf{I} )\\) must be equal to zero: \\[ | \\mathbf{A} - \\lambda \\mathbf{I} | = 0 \\] Notice that this equation turns into an equation in \\(\\lambda\\) only; so the eigenvalues can be found. In the example above, we have: \\[ | \\mathbf{A} - \\lambda \\mathbf{I} | = \\left | \\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; 1 \\\\ \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\right | = \\left | \\begin{matrix} 2 - \\lambda &amp; 3 \\\\ 4 &amp; 1 - \\lambda \\\\ \\end{matrix} \\right | = 0 \\] The determinant can be rewritten as: \\[ (2 - \\lambda) (1 - \\lambda) - 12 = 0 \\] or \\[ \\lambda^2 - 3\\lambda - 10 = 0 \\] with solutions \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = -2\\). Expanding the determinant \\(| \\mathbf{A} - \\lambda \\mathbf{I} |\\) becomes an equation of a \\(p\\)-th degree polynomial in \\(\\lambda\\) from which the values \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\) are the desired eigenvalues. This equation is the so-called characteristic equation, and it will have, in general, \\(p\\) different roots which are the eigenvalues. This means that finding eigenvalues is nothing else than finding the roots of a \\(p\\)-th degree polynomial. The problem is that, in general, the direct computation of the roots from the characteristic equation cannot be obtained in an analytical way. So how do people find the roots of the polynomial associated to the characteristic equation? Well, this is a problem that has called the attention of many mathematicians for many centuries. The bad news is that there is no general formula for the solution of the roots of a polynomial of degree greater than 4. The good news is that we can use iterative procedures based on the idea of successive approximation or linearization. Roughly speaking, the idea of the successive approximation involves starting from one or more initial approximations to a given root, so we can produce a sequence \\(l_0, l_1, l_2, \\dots\\) which presumably converges to the desired root. "],
["properties-of-eigenstructures.html", "12.2 Properties of Eigenstructures", " 12.2 Properties of Eigenstructures Knowing about the properties of eigenstructures will pay off when you encounter methods whose solutions involve an eigenvalue decomposition. Because all the following properties have the same premise, I prefer to state it here: suppose that matrix \\(\\mathbf{A}\\) has an eigenvalue \\(\\lambda\\) and an associated eigenvector \\(\\mathbf{v}\\): \\[ \\mathbf{Av} = \\lambda \\mathbf{v} \\] Prop. 1 The matrix \\(b \\mathbf{A}\\), where \\(b\\) is an arbitrary scalar, has \\(b \\lambda\\) as an eigenvalue, with \\(\\mathbf{v}\\) as the associated eigenvector. Prop. 2 The matrix \\(\\mathbf{B} = \\mathbf{A} + c \\mathbf{I}\\), where \\(c\\) is an arbitrary scalar, has \\((\\lambda + c)\\) as an eigenvalue, with \\(\\mathbf{v}\\) as the associated eigenvector. Prop. 3 The matrix \\(\\mathbf{A}^m\\), where \\(m\\) is any positive integer, has \\(\\lambda^m\\) as an eigenvalue, with \\(\\mathbf{v}\\) as the associated eigenvector. In other words, if \\(\\mathbf{v}\\) is an eigenvector of \\(\\mathbf{A}\\), then \\(\\mathbf{v}\\) is also an eigenvector of \\(\\mathbf{A}^2\\), \\(\\mathbf{A}^3\\), and so on. Likewise, \\(\\lambda^2\\) is an eigenvalue of \\(\\mathbf{A}^2\\), \\(\\lambda^3\\) is an eigenvalue of \\(\\mathbf{A}^3\\), etc. Prop. 4 The matrix \\(\\mathbf{A}^{-1}\\), assuming that \\(| \\mathbf{A} | \\neq 0\\), has \\(1 / \\lambda\\) as an eigenvalue, with \\(\\mathbf{v}\\) as the associated eigenvector. Also, when \\(\\mathbf{A}^{-1}\\) exists, then \\(\\mathbf{A}^{-p}\\) has the same eigenvectors as \\(\\mathbf{A}\\) and \\(\\lambda_{k}^{n}\\) is the \\(k\\)-th eigenvalue. In general, \\(\\mathbf{A}^{-1} \\mathbf{V} = \\mathbf{V} \\mathbf{\\Lambda}^{-1}\\). Prop. 5 If \\(\\mathbf{A}\\) is symmetric, then the eigenvectors of \\(\\mathbf{A}\\) are also eigenvectors of \\(\\mathbf{A^\\mathsf{T} A}\\). Prop. 6 The sum of the eigenvalues of a matrix is equal to the sum of the diagonal elements of the matrix (the trace of the matrix). \\[ tr(\\mathbf{A}) = \\sum_{k=1}^{n} \\lambda_k \\] Prop. 7 The product of the eigenvalues of \\(\\mathbf{A}\\) equals the determinant of \\(\\mathbf{A}\\): \\[ \\prod_{k=1}^{n} \\lambda_k = | \\mathbf{A} | \\] Prop. 8 If \\(\\mathbf{A}\\), a matrix of order \\(n \\times n\\), has rank \\(r\\), then \\(\\mathbf{A}\\) has \\(n - r\\) eigenvalues equal to zero. Prop. 9 If a symmetric matrix \\(\\mathbf{A}\\) can be written as the product \\(\\mathbf{A} = \\mathbf{V \\Lambda V^\\mathsf{T}}\\), where \\(\\mathbf{\\Lambda}\\) is diagonal with all entries nonnegative and \\(\\mathbf{V}\\) is an orthogonal matrix of eigenvectors, then: \\[ \\mathbf{A}^{1/2} = \\mathbf{V \\Lambda}^{1/2} \\mathbf{V} \\] and it is the case that \\(\\mathbf{A}^{1/2} \\mathbf{A}^{1/2} = \\mathbf{A}\\). Prop. 10 If a symmetric matrix \\(\\mathbf{A}^{-1}\\) can be written as the product \\(\\mathbf{A}^{-1} = \\mathbf{V \\Lambda}^{-1} \\mathbf{V^\\mathsf{T}}\\), where \\(\\mathbf{\\Lambda}^{-1}\\) is diagonal with all entries nonnegative and \\(\\mathbf{V}\\) is an orthogonal matrix of eigenvectors, then: \\[ \\mathbf{A}^{-1/2} = \\mathbf{V \\Lambda}^{-1/2} \\mathbf{V} \\] and it is the case that \\(\\mathbf{A}^{-1/2} \\mathbf{A}^{-1/2} = \\mathbf{A}^{-1}\\). 12.2.1 Symmetric Matrices The following table lists characteristics of eigenvalues depending on the type of symmetric matrices. Symmetric matrix Eigenvalues All elements of \\(\\mathbf{A}\\) are real All eigenvalues are real \\(\\mathbf{A}\\) is positive definite All eigenvalues are positive \\(\\mathbf{A}\\) is positive semidefinite and of rank \\(r\\) There are \\(r\\) positive and \\(p-r\\) null eigenvalues \\(\\mathbf{A}\\) is negative semidefinite and of rank \\(r\\) There are \\(r\\) negative and \\(p-r\\) null eigenvalues \\(\\mathbf{A}\\) is indefinite and of rank \\(r\\) There are \\(r\\) non-null and \\(n-r\\) null eigenvalues \\(\\mathbf{A}\\) is diagonal The diagonal elements are the eigenvalues "],
["power-method.html", "12.3 Power Method", " 12.3 Power Method Among all the set of methods which can be used to find eigenvalues and eigenvectors, one of the basic procedures following a successive approximation approach is the so-called Power Method. In its simplest form, the Power Method (PM) allows us to find the largest eigenvector and its corresponding eigenvalue. To be more precise, the PM allows us to find an approximation for the first eigenvalue of a symmetric matrix \\(\\mathbf{S}\\). The basic idea of the power method is to choose an arbitrary vector \\(\\mathbf{w_0}\\) to which we will apply the symmetric matrix \\(\\mathbf{S}\\) repeatedly to form the following sequence: \\[\\begin{align*} \\mathbf{w_1} &amp;= \\mathbf{S w_0} \\\\ \\mathbf{w_2} &amp;= \\mathbf{S w_1 = S^2 w_0} \\\\ \\mathbf{w_3} &amp;= \\mathbf{S w_2 = S^3 w_0} \\\\ \\vdots \\\\ \\mathbf{w_k} &amp;= \\mathbf{S w_{k-1} = S^k w_0} \\end{align*}\\] As you can see, the PM reduces to simply calculate the powers of \\(\\mathbf{S}\\) multiplied to the initial vector \\(\\mathbf{w_0}\\). Hence the name of power method. Figure 12.1: Illustration of the sequence of vectors in the Power Method In practice, we must rescale the obtained vector \\(\\mathbf{w_k}\\) at each step in order to avoid an eventual overflow or underflow. Also, the rescaling will allows us to judge whether the sequence is converging. Assuming a reasonable scaling strategy, the sequence of iterates will usually converge to the dominant eigenvector of \\(\\mathbf{S}\\). In other words, after some iterations, the vector \\(\\mathbf{w_{k-1}}\\) and \\(\\mathbf{w_k}\\) will be very similar, if not identical. The obtained vector is the dominant eigenvector. To get the corresponding eigenvalue we calculate the so-called Rayleigh quotient given by: \\[ \\lambda = \\frac{\\mathbf{w_{k}^{\\mathsf{T}} S^\\mathsf{T} w_k}}{\\| \\mathbf{w_k} \\|^2} \\] Figure 12.2: Sequence of vectors before and after scaling to unit norm 12.3.1 Power Method Algorithm There are some conditions for the power method to be succesfully used. One of them is that the matrix must have a dominant eigenvalue. The starting vector \\(\\mathbf{w_0}\\) must be nonzero. Very important, we need to scale each of the vectors \\(\\mathbf{w_k}\\), otherwise the algorithm will “explode”. The Power Method is of a striking simplicity. The only thing we need, computationally speaking, is the operation of matrix multiplication. Let’s consider a more detailed version of the PM algorithm walking through it step by step: Start with an arbitraty initial vector \\(\\mathbf{w}\\) obtain product \\(\\mathbf{\\tilde{w} = S w}\\) normalize \\(\\mathbf{\\tilde{w}}\\) \\[\\mathbf{w} = \\frac{\\mathbf{\\tilde{w}}}{\\| \\mathbf{\\tilde{w}} \\|}\\] compare \\(\\mathbf{w}\\) with its previous version repeat steps 2 till 4 until convergence 12.3.2 Convergence of the Power Method To see why and how the power method converges to the dominant eigenvalue, we need an important assumption. Let’s say the matrix \\(\\mathbf{S}\\) has \\(p\\) eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\), and that they are ordered in decreasing way \\(|\\lambda_1| &gt; |\\lambda_2| \\geq \\dots \\geq |\\lambda_p|\\). Note that the first eigenvalue is strictly greater than the second one. This is a very important assumption. In the same way, we’ll assume that the matrix \\(\\mathbf{S}\\) has \\(p\\) linearly independent vectors \\(\\mathbf{v_1}, \\dots, \\mathbf{v_p}\\) ordered in such a way that \\(\\mathbf{v_j}\\) corresponds to \\(\\lambda_j\\). The initial vector \\(\\mathbf{w_0}\\) may be expressed as a linear combination of \\(\\mathbf{v_1}, \\dots, \\mathbf{v_p}\\) \\[ \\mathbf{w_0} = a_1 \\mathbf{v_1} + \\dots + a_p \\mathbf{v_p} \\] At every step of the iterative process the vector \\(\\mathbf{w_m}\\) is given by: \\[ \\mathbf{S}^m = a_1 \\lambda_{1}^m \\mathbf{v_1} + \\dots + a_p \\lambda_{p}^m \\mathbf{v_p} \\] Since \\(\\lambda_1\\) is the dominant eigenvalue, the component in the direction of \\(\\mathbf{u_1}\\) becomes relatively greater than the other components as \\(m\\) increases. If we knew \\(\\lambda_1\\) in advance, we could rescale at each step by dividing by it to get: \\[ \\left(\\frac{1}{\\lambda_{1}^m}\\right) \\mathbf{S}^m = a_1 \\mathbf{v_1} + \\dots + a_p \\left(\\frac{\\lambda_{p}^m}{\\lambda_{1}^m}\\right) \\mathbf{v_p} \\] which converges to the eigenvector \\(a_1 \\mathbf{v_1}\\), provided that \\(a_1\\) is nonzero. Of course, in real life this scaling strategy is not possible—we don’t know \\(\\lambda_1\\). Consequenlty, the eigenvector is determined only up to a constant multiple, which is not a concern since the really important thing is the direction not the length of the vector. The speed of the convergence depends on how bigger \\(\\lambda_1\\) is respect with to \\(\\lambda_2\\), and on the choice of the initial vector \\(\\mathbf{w_0}\\). If \\(\\lambda_1\\) is not much larger than \\(\\lambda_2\\), then the convergence will be slow. One of the advantages of the power method is that it is a sequential method; this means that we can obtain \\(\\mathbf{w_1, w_2}\\), and so on, so that if we only need the first \\(k\\) vectors, we can stop the procedure at the desired stage. Once we’ve obtained the first eigenvector \\(\\mathbf{w_1}\\), we can compute the second vector by reducing the matrix \\(\\mathbf{S}\\) by the amount explained by the first principal component. This operation of reduction is called deflation and the residual matrix is obtained as: \\[ \\mathbf{E = S - z_{1}^{\\mathsf{T}} z_1} \\] where \\(\\mathbf{z_1} = \\mathbf{S v_1}\\). In order to calculate the second eigenvalue and its corresponding eigenvector, we operate on \\(\\mathbf{E}\\) in the same way as the operations on \\(\\mathbf{S}\\) to obtain \\(\\mathbf{w_2}\\). "],
["svd.html", "Chapter 13 Singular Value Decomposition", " Chapter 13 Singular Value Decomposition One of the most important decompositions in matrix algebra is known as the Singular Value Decomposition, commonly known as SVD. One of the reasons why this decomposition is such fundamental is because it can be applied to any type of matrix—rectangular or square, singular or nonsigular. In this chapter, you will learn about SVD and its role for statistical learning methods. "],
["svd-basics.html", "13.1 SVD Basics", " 13.1 SVD Basics The Singular Value Decomposition expresses any matrix, such as an \\(n \\times p\\) matrix \\(\\mathbf{X}\\), as the product of three other matrices: \\[ \\mathbf{X = U D V^\\mathsf{T}} \\] where: \\(\\mathbf{U}\\) is a \\(n \\times p\\) column orthonormal matrix containing the left singular vectors. \\(\\mathbf{D}\\) is a \\(p \\times p\\) diagonal matrix containing the singular values of \\(\\mathbf{X}\\). \\(\\mathbf{V}\\) is a \\(p \\times p\\) column orthonormal matrix containing the right singular vectors. In terms of the shapes of the matrices, the SVD decomposition has this form: \\[ \\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; \\mathbf{X} &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} = \\ \\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; \\mathbf{U} &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} \\ \\begin{bmatrix} &amp; &amp; \\\\ &amp; \\mathbf{D} &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} \\ \\begin{bmatrix} &amp; &amp; \\\\ &amp; \\mathbf{V}^\\mathsf{T} &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} \\] The SVD says that we can factorize \\(\\mathbf{X}\\) with the product of an orthonormal matrix \\(\\mathbf{U}\\), a diagonal matrix \\(\\mathbf{D}\\), and an orthonormal matrix \\(\\mathbf{V}\\). \\[ \\mathbf{X} = \\ \\begin{bmatrix} u_{11} &amp; \\cdots &amp; u_{1p} \\\\ u_{21} &amp; \\cdots &amp; u_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ u_{n1} &amp; \\cdots &amp; u_{np} \\\\ \\end{bmatrix} \\ \\begin{bmatrix} l_{1} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; l_{p} \\\\ \\end{bmatrix} \\ \\begin{bmatrix} v_{11} &amp; \\cdots &amp; v_{p1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ v_{1p} &amp; \\cdots &amp; v_{pp} \\\\ \\end{bmatrix} \\] 13.1.1 SVD Properties You can think of the SVD structure as the basic structure of a matrix. What does this mean? Well, to understand the meaning of basic structure, we need to say more things about what each of the matrices \\(\\mathbf{U}\\), \\(\\mathbf{D}\\), and \\(\\mathbf{V}\\) represent. To be more precise: About \\(\\mathbf{U}\\) The matrix \\(\\mathbf{U}\\) is the orthonormalized matrix which is the most basic component. It’s like the skeleton of the matrix. \\(\\mathbf{U}\\) is unitary, and its columns form a basis for the space spanned by the columns of \\(\\mathbf{X}\\). \\[ \\mathbf{U^\\mathsf{T} U} = \\mathbf{I}_{p} \\] \\(\\mathbf{U}\\) cannot be orthogonal (\\(\\mathbf{U U^\\mathsf{T} = I_n}\\)) unless \\(r = p\\) About \\(\\mathbf{V}\\) The matrix \\(\\mathbf{V}\\) is the orientation or correlational component. \\(\\mathbf{V}\\) is unitary, and its columns form a basis for the space spanned by the rows of \\(\\mathbf{X}\\). \\[ \\mathbf{V^\\mathsf{T} V} = \\mathbf{I}_{p} \\] \\(\\mathbf{V}\\) cannot be orthogonal (\\(\\mathbf{V V^\\mathsf{T} = I_p}\\)) unless \\(r = p = m\\) About \\(\\mathbf{D}\\) The matrix \\(\\mathbf{D}\\) is referred to as the spectrum and it is a scale component. Note that all the values in the diagonal of \\(\\mathbf{D}\\) are non-negative numbers. This matrix is also unique. It is a like a fingerprint of a matrix. It is assumed that the singular values are ordered from largest to smallest. All elements of \\(\\mathbf{D}\\) can be taken to be positive, ordered from large to small (with ties allowed). \\(\\mathbf{D}\\) has non-negative real numbers on the diagonal (assuming \\(\\mathbf{X}\\) is real). The rank of \\(\\mathbf{X}\\) is given by \\(r\\), the number of such positive values (which are called singular values). Furthermore, \\(r(\\mathbf{X}) \\leq min(n,p)\\). 13.1.2 Diagrams of SVD Under the standard convention that \\(n &gt; p\\), if we assume that \\(\\mathbf{X}\\) is of full-column rank \\(r(\\mathbf{X}) = p\\), then we can display the decomposition with the following diagram: Figure 13.1: SVD Decomposition Diagram In general, when the decomposed matrix \\(\\mathbf{X}\\) is not of full column-rank, that is \\(rank(\\mathbf{X}) = r &lt; p\\), then the diagram of SVD could be depicted as follows Figure 13.2: SVD Decomposition Diagram 13.1.3 Example Here’s an example of SVD in R, via the function svd(). First let’s create a matrix \\(\\mathbf{X}\\) with random numbers: # X matrix set.seed(22) X &lt;- matrix(rnorm(20), 5, 4) X [,1] [,2] [,3] [,4] [1,] -0.512 1.858 -0.764 -0.922 [2,] 2.485 -0.066 0.082 0.862 [3,] 1.008 -0.163 0.743 2.003 [4,] 0.293 -0.200 -0.084 0.937 [5,] -0.209 0.301 -0.793 -1.616 R comes with the function svd(); it’s output is a list with three elements: # singular value decomposition SVD &lt;- svd(X) # elements returned by svd() names(SVD) [1] &quot;d&quot; &quot;u&quot; &quot;v&quot; d is a vector containing the singular values (i.e. values in the diagonal of \\(\\mathbf{D}\\)) u is the matrix of left singular values. v is the matrix of right singular values. # vector of singular values (d &lt;- SVD$d) [1] 3.952 2.022 1.475 0.432 # matrix of left singular vectors (U = SVD$u) [,1] [,2] [,3] [,4] [1,] -0.425 -0.5391 -0.723 0.00979 [2,] 0.527 -0.7686 0.286 0.05610 [3,] 0.575 0.0500 -0.442 0.13107 [4,] 0.222 0.0527 -0.170 -0.95123 [5,] -0.402 -0.3366 0.413 -0.27337 # matrix of right singular vectors (V = SVD$v) [,1] [,2] [,3] [,4] [1,] 0.571 -0.741 0.3386 0.104 [2,] -0.274 -0.530 -0.7680 0.234 [3,] 0.277 0.321 -0.0446 0.905 [4,] 0.723 0.261 -0.5418 -0.341 Let’s check that \\(\\mathbf{X} = \\mathbf{U D V^\\mathsf{T}}\\) # X equals U D V&#39; U %*% diag(d) %*% t(V) [,1] [,2] [,3] [,4] [1,] -0.512 1.858 -0.764 -0.922 [2,] 2.485 -0.066 0.082 0.862 [3,] 1.008 -0.163 0.743 2.003 [4,] 0.293 -0.200 -0.084 0.937 [5,] -0.209 0.301 -0.793 -1.616 # compare to X X [,1] [,2] [,3] [,4] [1,] -0.512 1.858 -0.764 -0.922 [2,] 2.485 -0.066 0.082 0.862 [3,] 1.008 -0.163 0.743 2.003 [4,] 0.293 -0.200 -0.084 0.937 [5,] -0.209 0.301 -0.793 -1.616 Let’s also confirm that \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal: # U orthonormal (U&#39;U = I) t(U) %*% U [,1] [,2] [,3] [,4] [1,] 1.00e+00 1.39e-16 2.78e-17 0.00e+00 [2,] 1.39e-16 1.00e+00 -2.78e-17 -8.33e-17 [3,] 2.78e-17 -2.78e-17 1.00e+00 5.55e-17 [4,] 0.00e+00 -8.33e-17 5.55e-17 1.00e+00 # V orthonormal (V&#39;V = I) t(V) %*% V [,1] [,2] [,3] [,4] [1,] 1.00e+00 -1.11e-16 -5.55e-17 1.11e-16 [2,] -1.11e-16 1.00e+00 8.33e-17 1.94e-16 [3,] -5.55e-17 8.33e-17 1.00e+00 -8.33e-17 [4,] 1.11e-16 1.94e-16 -8.33e-17 1.00e+00 13.1.4 Relation of SVD and Cross-Product Matrices If you consider \\(\\mathbf{X}\\) to be a data matrix of \\(n\\) individuals and \\(p\\) variables, you should be able to recall that we can obtain two cross-products: \\(\\mathbf{X^\\mathsf{T}X}\\) and \\(\\mathbf{X X^\\mathsf{T}}\\). It turns out that we can use the singular value decomposition of \\(\\mathbf{X}\\) to find the corresponding factorization of each of these products. The cross-product matrix of columns can be expressed as: \\[\\begin{align*} \\mathbf{X^\\mathsf{T} X} &amp;= (\\mathbf{U D V^\\mathsf{T}})^\\mathsf{T} (\\mathbf{U D V^\\mathsf{T}}) \\\\ &amp;= (\\mathbf{V D U^\\mathsf{T}}) (\\mathbf{U D V^\\mathsf{T}}) \\\\ &amp;= \\mathbf{V D} (\\mathbf{U^\\mathsf{T}} \\mathbf{U}) \\mathbf{D V^\\mathsf{T}} \\\\ &amp;= \\mathbf{V D^2 V^\\mathsf{T}} \\end{align*}\\] The cross-product matrix of rows can be expressed as: \\[\\begin{align*} \\mathbf{X X^\\mathsf{T}} &amp;= (\\mathbf{U D V^\\mathsf{T}}) (\\mathbf{U D V^\\mathsf{T}})^\\mathsf{T} \\\\ &amp;= (\\mathbf{U D V^\\mathsf{T}}) (\\mathbf{V D U^\\mathsf{T}}) \\\\ &amp;= \\mathbf{U D} (\\mathbf{V^\\mathsf{T}} \\mathbf{V}) \\mathbf{D U^\\mathsf{T}} \\\\ &amp;= \\mathbf{U D^2 U^\\mathsf{T}} \\end{align*}\\] One of the interesting things about SVD is that \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are matrices whose columns are eigenvectors of product moment matrices that are derived from \\(\\mathbf{X}\\). Specifically, \\(\\mathbf{U}\\) is the matrix of eigenvectors of (symmetric) \\(\\mathbf{XX^\\mathsf{T}}\\) of order \\(n \\times n\\) \\(\\mathbf{V}\\) is the matrix of eigenvectors of (symmetric) \\(\\mathbf{X^\\mathsf{T}X}\\) of order \\(p \\times p\\) Of additional interest is the fact that \\(\\mathbf{D}\\) is a diagonal matrix whose main diagonal entries are the square roots of \\(\\mathbf{U}^2\\), the common matrix of eigenvalues of \\(\\mathbf{XX^\\mathsf{T}}\\) and \\(\\mathbf{X^\\mathsf{T}X}\\). The EVD of the cross-product matrix of columns (or minor product moment) \\(\\mathbf{X^\\mathsf{T} X}\\) can be expressed as: \\[ \\mathbf{X^\\mathsf{T} X} = \\mathbf{V \\Lambda V^\\mathsf{T}} \\] in terms of the SVD factorization of \\(\\mathbf{X}\\): \\[ \\mathbf{X^\\mathsf{T} X} = \\mathbf{V D^2 V^\\mathsf{T}} \\] The EVD of the cross-product matrix of rows (or major product moment) \\(\\mathbf{X X^\\mathsf{T}}\\) can be expressed as: \\[ \\mathbf{X X^\\mathsf{T}} = \\mathbf{U \\Lambda U^\\mathsf{T}} \\] in terms of the SVD factorization of \\(\\mathbf{X}\\): \\[ \\mathbf{X X^\\mathsf{T}} = \\mathbf{U D^2 U^\\mathsf{T}} \\] "],
["svd-rank-reduction-theorem.html", "13.2 SVD Rank-Reduction Theorem", " 13.2 SVD Rank-Reduction Theorem A very interesting and alternative way to represent the SVD is with the following formula: \\[ \\mathbf{X} = \\sum_{k=1}^{p} l_k \\mathbf{u_k} \\mathbf{v^\\mathsf{T}_k} \\] This equation expresses the SVD as a sum of \\(p\\) rank 1 matrices. This result is formalized in what is known as the SVD theorem described by Carl Eckart and Gale Young in 1936, and it is often referred to as the Eckart-Young theorem. This theorem applies to practically any arbitrary rectangular matrix. The SVD theorem of Eckart and Young is related to the important problem of approximating a matrix. The basic result says that if \\(\\mathbf{X}\\) is an \\(n \\times p\\) rectangular matrix, then the best \\(r\\)-dimensional approximation \\(\\hat{\\mathbf{X}}\\) to \\(\\mathbf{X}\\) is obtained by minimizing: \\[ min \\| \\mathbf{X - \\hat{X}} \\|^2 \\] This type of approximation is a least squares approximation and the solution is obtained by taking the first \\(r\\) elements of matrices \\(\\mathbf{U}, \\mathbf{D}, \\mathbf{V}\\) so that the \\(n \\times r\\) matrix \\[ \\mathbf{\\hat{X} = U_r D_r V^\\mathsf{T}_r} \\] The SVD theorem says that any rectangular matrix \\(\\mathbf{X}\\) can be broken down into the product of three matrices—an orthogonal matrix \\(\\mathbf{U}\\), a diagonal matrix \\(\\mathbf{D}\\), and the transpose of an orthogonal matrix \\(\\mathbf{V}\\). In terms of the diagonal elements \\(l_1, l_2, \\dots, l_r\\) of \\(\\mathbf{D}\\), the columns \\(\\mathbf{u_1}, \\dots, \\mathbf{u_r}\\) of \\(\\mathbf{U}\\), and the columns \\(\\mathbf{v_1}, \\dots, \\mathbf{v_r}\\) of \\(\\mathbf{V}\\), the basic structure of \\(\\mathbf{X}\\) may be written as \\[ \\mathbf{X} = l_1 \\mathbf{u_1 v^\\mathsf{T}_1} + l_2 \\mathbf{u_2 v^\\mathsf{T}_2} + \\dots + l_r \\mathbf{u_r v^\\mathsf{T}_r} \\] which shows that the matrix \\(\\mathbf{X}\\) of rank \\(r\\) is a linear combination of \\(r\\) matrices of rank 1. Figure 13.3: SVD as a sum of rank-one matrices For eample, if you take only the first two singular vectors and values, you can obtain a two-rank approximation \\(\\mathbf{\\hat{X}}\\) of \\(\\mathbf{X}\\). \\[ \\mathbf{X} \\approx \\mathbf{\\hat{X}_{2}} = l_1 \\mathbf{u_1} \\mathbf{v^\\mathsf{T}_1} + l_2 \\mathbf{u_2} \\mathbf{v^\\mathsf{T}_2} \\] Figure 13.4: SVD rank-two approximation If you think about it, the SVD is of great utility because it tells us that the best 1-rank approximation, in the least squares sense, of any matrix \\(\\mathbf{X}\\) is \\(l_1 \\mathbf{u_1} \\mathbf{v^\\mathsf{T}_1}\\). \\[ \\mathbf{X} \\approx \\mathbf{\\hat{X}_1} = l_1 \\mathbf{u_1} \\mathbf{v^\\mathsf{T}_1} \\] This implies that, from a conceptual standpoint, we can approximate the \\(n \\times p\\) numbers in \\(\\mathbf{X}\\) with just \\(n + p + 1\\) values: \\(n\\) numbers in \\(\\mathbf{u_1}\\), \\(p\\) numbers in \\(\\mathbf{v_1}\\), and one scalar \\(l_1\\). "]
]
